{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "becoming-coalition",
   "metadata": {},
   "source": [
    "# <font color='red'>1.0 IMPORT</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "collectible-company",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.729970Z",
     "start_time": "2021-10-01T23:04:35.304216Z"
    }
   },
   "outputs": [],
   "source": [
    "# ENVIRONMENT\n",
    "\n",
    "# !pip install mysql-connector-python\n",
    "# !pip install pymysql\n",
    "# !pip install psycopg2\n",
    "# !pip install sqlalchemy\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from   datetime import datetime\n",
    "import mysql.connector\n",
    "import numpy as np\n",
    "import sqlalchemy as sa \n",
    "\n",
    "# pd.options.display.max_rows = 2000\n",
    "# pd.options.display.width = 120\n",
    "# pd.options.display.max_colwidth = 100\n",
    "\n",
    "ETL_VERSAO = 'etl_v1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b130e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.735072Z",
     "start_time": "2021-10-01T23:04:35.731966Z"
    }
   },
   "outputs": [],
   "source": [
    "# escolhe qual ambiente será carregado\n",
    "# {'chave': ('nome do ambiente', 'in', 'out')}\n",
    "ambientes = {\n",
    "    0: ('produção',        'bd_fontes',     'bd_dw'),\n",
    "    1: ('homologação',     'bd_hom_fontes', 'bd_hom_dw'),\n",
    "    2: ('desenvolvimento', 'bd_dev_fontes', 'bd_dev_dw'),\n",
    "    3: ('simulação',       'tt_fontes',     'tt_dw'),\n",
    "} \n",
    "\n",
    "# selecione o banco em questão\n",
    "amb = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-collar",
   "metadata": {},
   "source": [
    "# <font color='red'>2.0 EXTRACT</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8aa9d10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:52:50.721375Z",
     "start_time": "2021-10-01T23:52:50.714048Z"
    }
   },
   "outputs": [],
   "source": [
    "# tipo 0-normal, 1-sqlalchemy\n",
    "def ConectaBD(bd, tipo):\n",
    "    \n",
    "    s_host = 'localhost'\n",
    "    s_bd   = bd\n",
    "    s_user = 'gd' \n",
    "    s_pw   = 'Cavaquinho@Dourado@6390'\n",
    "    \n",
    "    if tipo == 0:\n",
    "        cn = mysql.connector.connect(\n",
    "            host        = s_host, \n",
    "            database    = s_bd, \n",
    "            user        = s_user, \n",
    "            password    = s_pw,\n",
    "            auth_plugin = 'mysql_native_password'\n",
    "        )\n",
    "    elif tipo == 1:\n",
    "        st = sa.engine.url.URL.create(\n",
    "            drivername ='mysql+pymysql',\n",
    "            username    =s_user,\n",
    "            password    =s_pw,\n",
    "            host        =s_host,\n",
    "#             port='3307',\n",
    "            database    =s_bd,\n",
    "        )\n",
    "        eng = sa.create_engine(st)\n",
    "        cn = eng.connect()\n",
    "    else:\n",
    "        cn = None\n",
    "        \n",
    "    return cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25351fae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.748424Z",
     "start_time": "2021-10-01T23:04:35.745508Z"
    }
   },
   "outputs": [],
   "source": [
    "def OLD_ConectaBD(bd):\n",
    "    \n",
    "    cn = mysql.connector.connect(\n",
    "        host        = 'localhost', \n",
    "        database    = bd, \n",
    "        user        = 'gd', \n",
    "        password    = 'Cavaquinho@Dourado@6390',\n",
    "        auth_plugin = 'mysql_native_password'\n",
    "    )\n",
    "    return cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "blocked-hanging",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:07:12.919282Z",
     "start_time": "2021-10-01T23:07:12.912631Z"
    }
   },
   "outputs": [],
   "source": [
    "def LeFontes():\n",
    "\n",
    "    print('Iniciando leitura de dados nativos...', end='')\n",
    "    \n",
    "    sql_form = (\n",
    "    \"SELECT * \"\n",
    "    \"FROM form\"\n",
    "    )\n",
    "    sql_tasks = (\n",
    "    \"SELECT DISTINCT `Protocolo`, `Entidade`, `Serviço`, `Usuário`, `Grupo`, `Data e Hora de conclusão`, \"\n",
    "    \"`Data e Hora de criação`, `Ação`, `Encaminhado para`, `Processo encerrado`, `Processo cancelado`, \"\n",
    "    \"`Motivo de cancelamento`, `Status externo`, `Categoria`, `Grupo responsável`, `Prazo (em segundos)` \"\n",
    "    \"FROM tasks\"\n",
    "    )\n",
    "    sql_sla = (\n",
    "    \"SELECT * \"\n",
    "    \"FROM sla\"\n",
    "    )\n",
    "    sql_rating = (\n",
    "    \"SELECT * \"\n",
    "    \"FROM rating\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        dbConn = ConectaBD(ambientes[amb][1], 0)\n",
    "\n",
    "        df_int_tasks =   pd.read_sql(sql_tasks,  con=dbConn)\n",
    "        df_int_sla =     pd.read_sql(sql_sla,    con=dbConn)\n",
    "        df_int_form =    pd.read_sql(sql_form,   con=dbConn)\n",
    "        df_int_rating =  pd.read_sql(sql_rating, con=dbConn)\n",
    "        \n",
    "        print('OK')\n",
    "\n",
    "    except mysql.connector.Error as error:\n",
    "        print(\"Failed to read record from MySQL table {}\".format(error))\n",
    "\n",
    "    finally:\n",
    "        if (dbConn.is_connected()):\n",
    "            dbConn.close()\n",
    "            print(f'tasks:{df_int_tasks.shape[0]} registros lidos em {df_int_tasks.shape[1]} colunas')\n",
    "            print(f'sla: {df_int_sla.shape[0]} registros lidos em {df_int_sla.shape[1]} colunas')\n",
    "            print(f'form: {df_int_form.shape[0]} registros lidos em {df_int_form.shape[1]} colunas')\n",
    "            print(f'rating: {df_int_rating.shape[0]} registros lidos em {df_int_rating.shape[1]} colunas')\n",
    "            print(\"MySQL connection is closed\")\n",
    "            \n",
    "    return df_int_tasks, df_int_sla, df_int_form, df_int_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-river",
   "metadata": {},
   "source": [
    "# <font color='red'>3.0 TRANSFORM</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-providence",
   "metadata": {},
   "source": [
    "## <font color='blue'>trata os datasets</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-night",
   "metadata": {},
   "source": [
    "### <font color='black'>trata o dataset sla</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "weekly-martial",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.767295Z",
     "start_time": "2021-10-01T23:04:35.758948Z"
    }
   },
   "outputs": [],
   "source": [
    "def trSLA(df):\n",
    "    \n",
    "    print('df_sla - iniciando transformação... ', end='')\n",
    "    \n",
    "    MINUTOS_PARA_DIAS = 1440 # quantidade de minutos em um dia\n",
    "\n",
    "    # preenche com null as colunas que contém 'null' como tipo string\n",
    "    df.loc[(df['limiteMinimo'] == ''), 'limiteMinimo'] = np.nan\n",
    "    df.loc[(df['limiteMaximo'] == ''), 'limiteMaximo'] = np.nan\n",
    "\n",
    "    # transforma o tipo de coluna para float\n",
    "    df['limiteMinimo'] = df['limiteMinimo'].astype(float)\n",
    "    df['limiteMaximo'] = df['limiteMaximo'].astype(float)\n",
    "\n",
    "    # transforma a unidade de medida de minuto para dias\n",
    "    df['limiteMinimo'] = df['limiteMinimo'] / MINUTOS_PARA_DIAS\n",
    "    df['limiteMaximo'] = df['limiteMaximo'] / MINUTOS_PARA_DIAS\n",
    "\n",
    "    # preenche com valores extremos os limites máximos e limites mínimos\n",
    "    df.loc[(df['limiteMinimo'].isna()), 'limiteMinimo'] = -9999999.9\n",
    "    df.loc[(df['limiteMaximo'].isna()), 'limiteMaximo'] = 9999999.9\n",
    "\n",
    "    # cria mais uma coluna de status para pivotar limite mínimo e limite máxio\n",
    "    df['status2'] = df['status']\n",
    "    df.rename(columns={'status': 'StatusLmin', 'status2': 'StatusLmax'}, inplace = True)\n",
    "    \n",
    "    # faz pivot da coluna StatusLmin\n",
    "    idx = ['entityCode', 'service', 'StatusLmax', 'limiteMaximo']\n",
    "    df = df.pivot(columns = 'StatusLmin', values = 'limiteMinimo', index=idx).reset_index()\n",
    "    df.columns.name = None\n",
    "    dic_renome = {'Dentro do prazo' : 'Dentro do Prazo LMin', \n",
    "                  'Fora do prazo' : 'Fora do Prazo LMin',\n",
    "                  'Perto do prazo' : 'Perto do Prazo LMin'}\n",
    "    df.rename(columns=dic_renome, inplace = True)\n",
    "\n",
    "    # faz pivot da coluna StatusLmax\n",
    "    idx = ['entityCode', 'service', 'Dentro do Prazo LMin', 'Fora do Prazo LMin', 'Perto do Prazo LMin']\n",
    "    df = df.pivot(columns = 'StatusLmax', values = 'limiteMaximo', index=idx).reset_index()\n",
    "    df.columns.name = None\n",
    "    dic_renome = {'Dentro do prazo' : 'Dentro do Prazo LMax', \n",
    "                  'Fora do prazo' : 'Fora do Prazo LMax',\n",
    "                  'Perto do prazo' : 'Perto do Prazo LMax'}\n",
    "    df.rename(columns=dic_renome, inplace = True)\n",
    "\n",
    "    # agrupa por entidade e serviço\n",
    "    df = df.groupby('service').sum().reset_index()\n",
    "\n",
    "    print('%d linhas OK.' %df.shape[0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-pierre",
   "metadata": {},
   "source": [
    "### <font color='black'>trata o dataset form</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "scheduled-auction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.773321Z",
     "start_time": "2021-10-01T23:04:35.769590Z"
    }
   },
   "outputs": [],
   "source": [
    "def trForm(df):\n",
    "    \n",
    "    print('df_form - iniciando transformação... ', end='')\n",
    "\n",
    "    df.columns = ['atributo', 'nome', 'valor', 'protocolo', 'servico', 'tipo', 'campo_relacionado']\n",
    "    df['atributo'] = df['atributo'].str.lower()\n",
    "    \n",
    "    print('%d linhas OK.' %df.shape[0])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-yukon",
   "metadata": {},
   "source": [
    "### <font color='black'>trata o dataset rating</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dynamic-jason",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.780761Z",
     "start_time": "2021-10-01T23:04:35.775417Z"
    }
   },
   "outputs": [],
   "source": [
    "def trRating(df):\n",
    "\n",
    "    print('df_rating - iniciando transformação... ', end='')\n",
    "\n",
    "    # renomeia as colunas\n",
    "    cols = ['Solicitacao', 'NotaAvaliacao', 'MotivoAvaliacao', 'DataHoraAvaliacao']\n",
    "    df.columns = cols\n",
    "\n",
    "    # separa e formata as colunas de datas e horas \n",
    "    df['DataAvaliacao'] = pd.to_datetime(df['DataHoraAvaliacao']).dt.date\n",
    "    df['HoraAvaliacao'] = pd.to_datetime(df['DataHoraAvaliacao']).dt.time\n",
    "\n",
    "    # deleta a coluna que contém data e hora\n",
    "    df.drop(['DataHoraAvaliacao'], axis=1, inplace=True)\n",
    "\n",
    "    # preenche colunas de linhas vazias\n",
    "    df.loc[df['MotivoAvaliacao'] == '', 'MotivoAvaliacao'] = '<motivo vazio>'\n",
    "    \n",
    "    print('%d linhas OK.' %df.shape[0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-democrat",
   "metadata": {},
   "source": [
    "### <font color='black'>trata o dataset tasks</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "assumed-blast",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.791069Z",
     "start_time": "2021-10-01T23:04:35.783299Z"
    }
   },
   "outputs": [],
   "source": [
    "def trTasks(df):\n",
    "\n",
    "    print('df_tasks - iniciando transformação... ', end='')\n",
    "\n",
    "    SEM_STATUS = '<sem status inicial>'\n",
    "\n",
    "    # renomeia as colunas\n",
    "    lst_colunas_tasks = ['Protocolo', 'Entidade', 'Servico', 'Usuarios','Grupo', 'DataHora_Conclusao',\n",
    "                         'DataHora_Criacao', 'Acao', 'EncaminhadoPara', 'ProcessoEncerrado', 'ProcessoCancelado',\n",
    "                         'MotivoCancelamento', 'StatusExterno', 'Categoria', 'GrupoResponsavel','Prazo']\n",
    "    df.columns = lst_colunas_tasks\n",
    "\n",
    "    # coloca null em todas as colunas com dados vazios ou null\n",
    "    df = df.replace(['null', '', '-'], np.nan)\n",
    "\n",
    "    # coloca dataset em ordem para acertar o status externo\n",
    "    lst = ['Entidade', 'Protocolo', 'DataHora_Criacao']\n",
    "    df = df.sort_values(by=lst).reset_index(drop=True)\n",
    "\n",
    "    # acerta os tipos das colunas datetime\n",
    "    df['DataHora_Conclusao'] = df['DataHora_Conclusao'].astype('datetime64')\n",
    "    df['DataHora_Criacao']   = df['DataHora_Criacao'].astype('datetime64')\n",
    "\n",
    "    # rotina para preencher status externo vazio\n",
    "    # pega sempre o anterior e se o primeiro status estiver vazio coloca \"sem status inicial\"\n",
    "    if pd.isna(df.loc[0, 'StatusExterno']):\n",
    "        df.loc[0, 'StatusExterno'] = SEM_STATUS\n",
    "\n",
    "    for i in range(1, df.shape[0]):\n",
    "        if pd.isna(df.loc[i, 'StatusExterno']):    \n",
    "            if df.loc[i, 'Protocolo'] == df.loc[i - 1, 'Protocolo']:\n",
    "                df.loc[i, 'StatusExterno'] = df.loc[i - 1, 'StatusExterno']\n",
    "            else:\n",
    "                df.loc[i, 'StatusExterno'] = SEM_STATUS\n",
    "\n",
    "    print('%d linhas OK.' %df.shape[0])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-calgary",
   "metadata": {},
   "source": [
    "## <font color='blue'>monta tabelas dimensões</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-interim",
   "metadata": {},
   "source": [
    "### DIM acoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "digital-library",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.802219Z",
     "start_time": "2021-10-01T23:04:35.796814Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_dim_Acoes(df):\n",
    "\n",
    "    print('dim_Acoes - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna Acao em uma coluna do tipo \"category'\n",
    "    df['tmpAcao'] = df['Acao'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Ação\n",
    "    df['FK_dim_Acoes'] = df['tmpAcao'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_Acoes', 'Acao']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_Acao': 'PK_dim_Acao'}, inplace = True)\n",
    "    \n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['Acao'] == '', ['Acao']] = '<sem ação determinada>'\n",
    "    \n",
    "    # exclui a coluna Acoes de df_tasks que será a futura tabela fato\n",
    "    df.drop(['Acao', 'tmpAcao'], axis=1, inplace=True)\n",
    "    \n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-juice",
   "metadata": {},
   "source": [
    "### DIM categoria servicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "medium-teaching",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.809734Z",
     "start_time": "2021-10-01T23:04:35.804381Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_dim_CategoriaServico(df):\n",
    "\n",
    "    print('dim_Categoria - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna Categoria em uma coluna do tipo \"category'\n",
    "    df['tmpCategoria'] = df['Categoria'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão CategoriasServicos\n",
    "    df['FK_dim_CategoriasServicos'] = df['tmpCategoria'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_CategoriasServicos', 'Categoria']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_CategoriasServicos': 'PK_dim_CategoriasServicos'}, inplace = True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['Categoria'] == '', ['Categoria']] = '<categoria indefinida>'\n",
    "    \n",
    "    # exclui a coluna Categoria de df_tasks que será a futura tabela fato\n",
    "    df.drop(['Categoria', 'tmpCategoria'], axis=1, inplace=True)\n",
    "    \n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-things",
   "metadata": {},
   "source": [
    "### DIM encaminhamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "comic-cookbook",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.817350Z",
     "start_time": "2021-10-01T23:04:35.811740Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_dim_Encaminhamento(df):\n",
    "\n",
    "    print('dim_Encaminhamento - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna EncaminhadoPara em uma coluna do tipo \"category'\n",
    "    df['tmpEncaminhadoPara'] = df['EncaminhadoPara'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Encaminhamento\n",
    "    df['FK_dim_Encaminhamento'] = df['tmpEncaminhadoPara'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_Encaminhamento', 'EncaminhadoPara']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_Encaminhamento': 'PK_dim_Encaminhamento'}, inplace = True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['EncaminhadoPara'] == '', ['EncaminhadoPara']] = '<sem encaminhamento>'\n",
    "\n",
    "    # exclui a coluna Entidade de df_tasks que será a futura tabela fato\n",
    "    df.drop(['EncaminhadoPara', 'tmpEncaminhadoPara'], axis=1, inplace=True)\n",
    "\n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-portland",
   "metadata": {},
   "source": [
    "### DIM entidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "extraordinary-hungary",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.825886Z",
     "start_time": "2021-10-01T23:04:35.820128Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_dim_Entidade(df):\n",
    "\n",
    "    print('dim_Entidade - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna Entidade em uma coluna do tipo \"category'\n",
    "    df['tmpEntidade'] = df['Entidade'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Entidades\n",
    "    df['FK_dim_Entidades'] = df['tmpEntidade'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_Entidades', 'Entidade']].drop_duplicates()\n",
    "    dfx['Entidade'] = dfx['Entidade'].str.upper()\n",
    "    dfx.rename(columns={'FK_dim_Entidades': 'PK_dim_Entidades'}, inplace = True)\n",
    "\n",
    "    # exclui a coluna Entidade de df_tasks que será a futura tabela fato\n",
    "    df.drop(['Entidade', 'tmpEntidade'], axis=1, inplace=True)\n",
    "\n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-silicon",
   "metadata": {},
   "source": [
    "### DIM grupo responsavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "central-camera",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.834756Z",
     "start_time": "2021-10-01T23:04:35.828366Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_dim_GrupoResponsavel(df):\n",
    "\n",
    "    print('dim_GrupoResponsavel - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna GrupoResponsavel em uma coluna do tipo \"category'\n",
    "    df['tmpGrupoResponsavel'] = df['GrupoResponsavel'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão GrupoResponsavel\n",
    "    df['FK_dim_GrupoResponsavel'] = df['tmpGrupoResponsavel'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_GrupoResponsavel', 'GrupoResponsavel']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_GrupoResponsavel': 'PK_dim_GrupoResponsavel'}, inplace = True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['GrupoResponsavel'] == '', ['GrupoResponsavel']] = '<grupo responsável não definido>'\n",
    "\n",
    "    # exclui a coluna GrupoResponsavel de df_tasks que será a futura tabela fato\n",
    "    df.drop(['GrupoResponsavel', 'tmpGrupoResponsavel'], axis=1, inplace=True)\n",
    "    \n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-sample",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-04T11:19:20.012078Z",
     "start_time": "2021-03-04T11:19:20.009504Z"
    }
   },
   "source": [
    "### DIM grupo usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "charged-france",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.842543Z",
     "start_time": "2021-10-01T23:04:35.837366Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_dim_GruposUsuarios(df):\n",
    "\n",
    "    print('dim_GruposUsuarios - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna Grupo em uma coluna do tipo \"category'\n",
    "    df['tmpGrupo'] = df['Grupo'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Grupo\n",
    "    df['FK_dim_GruposUsuarios'] = df['tmpGrupo'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_GruposUsuarios', 'Grupo']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_GruposUsuarios': 'PK_dim_GruposUsuarios'}, inplace = True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['Grupo'] == '', ['Grupo']] = '<grupo usuário não definido>'\n",
    "\n",
    "    # exclui a coluna Grupo de df_tasks que será a futura tabela fato\n",
    "    df.drop(['Grupo', 'tmpGrupo'], axis=1, inplace=True)\n",
    "    \n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-grenada",
   "metadata": {},
   "source": [
    "### DIM motivos cancelamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "greatest-champagne",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.850343Z",
     "start_time": "2021-10-01T23:04:35.844604Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_MotivosCanc(df):\n",
    "\n",
    "    print('dim_MotivosCanc - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna MotivoCancelamento em uma coluna do tipo \"category'\n",
    "    df['tmpMotivoCancelamento'] = df['MotivoCancelamento'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão MotivoCanc\n",
    "    df['FK_dim_MotivosCanc'] = df['tmpMotivoCancelamento'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_MotivosCanc', 'MotivoCancelamento']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_MotivosCanc': 'PK_dim_MotivosCanc'}, inplace = True)\n",
    "\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['MotivoCancelamento'] == '', ['MotivoCancelamento']] = '<sem motivo de cancelamento>'\n",
    "\n",
    "    # exclui a coluna MotivoCancelamento de df_tasks que será a futura tabela fato\n",
    "    df.drop(['MotivoCancelamento', 'tmpMotivoCancelamento'], axis=1, inplace=True)\n",
    "\n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-barbados",
   "metadata": {},
   "source": [
    "### DIM servico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "weekly-universal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.858782Z",
     "start_time": "2021-10-01T23:04:35.852593Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_Servico(df, dff_sla):\n",
    "\n",
    "    print('dim_Servico - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna Servico em uma coluna do tipo \"category'\n",
    "    df['tmpServico'] = df['Servico'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Servico\n",
    "    df['FK_dim_Servicos'] = df['tmpServico'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_Servicos', 'Servico']].drop_duplicates()\n",
    "\n",
    "    # exclui a coluna Servico de df_tasks que será a futura tabela fato\n",
    "    df.drop(['Servico', 'tmpServico'], axis=1, inplace=True)\n",
    "\n",
    "    # defive os nomes das colunas de SLA\n",
    "    cols = {'FK_dim_Servicos': 'PK_dim_Servicos',\n",
    "            'Dentro do Prazo LMin': 'sla_VD_Lmin',\n",
    "            'Perto do Prazo LMin': 'sla_AM_Lmin',\n",
    "            'Fora do Prazo LMin': 'sla_VM_Lmin',\n",
    "            'Dentro do Prazo LMax': 'sla_VD_Lmax',\n",
    "            'Perto do Prazo LMax': 'sla_AM_LMax',\n",
    "            'Fora do Prazo LMax': 'sla_VM_LMax',\n",
    "           }\n",
    "    # faz um merge da dimensão dim_Servicos com a tabela de SLAs\n",
    "    dfx = dfx.merge(dff_sla, left_on='Servico', right_on='service', how='left')\n",
    "    dfx = dfx.drop('service', axis=1)\n",
    "    dfx.rename(columns=cols, inplace = True)\n",
    "    \n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-occurrence",
   "metadata": {},
   "source": [
    "### DIM status externo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "prompt-rebecca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.866360Z",
     "start_time": "2021-10-01T23:04:35.861016Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_StatusExt(df):\n",
    "\n",
    "    print('dim_StatusExt - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna StatusExterno em uma coluna do tipo \"category'\n",
    "    df['tmpStatusExterno'] = df['StatusExterno'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Grupo\n",
    "    df['FK_dim_StatusExt'] = df['tmpStatusExterno'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_StatusExt', 'StatusExterno']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_StatusExt': 'PK_dim_StatusExt'}, inplace = True)\n",
    "\n",
    "    # exclui a coluna Grupo de df_tasks que será a futura tabela fato\n",
    "    df.drop(['StatusExterno', 'tmpStatusExterno'], axis=1, inplace=True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['StatusExterno'] == '', ['StatusExterno']] = '<sem status>'\n",
    "\n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-textbook",
   "metadata": {},
   "source": [
    "### DIM usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "connected-theology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.873940Z",
     "start_time": "2021-10-01T23:04:35.868352Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_Usuarios(df):\n",
    "\n",
    "    print('dim_Usuario - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna Usuario em uma coluna do tipo \"category'\n",
    "    df['tmpUsuario'] = df['Usuarios'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Usuario\n",
    "    df['FK_dim_Usuarios'] = df['tmpUsuario'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_Usuarios', 'Usuarios']].drop_duplicates()\n",
    "    col_ren = {'FK_dim_Usuarios': 'PK_dim_Usuarios', 'Usuarios': 'Usuario'}\n",
    "    dfx.rename(columns=col_ren, inplace = True)\n",
    "\n",
    "    # exclui a coluna Usuario de df_tasks que será a futura tabela fato\n",
    "    df.drop(['Usuarios', 'tmpUsuario'], axis=1, inplace=True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['Usuario'] == '', ['Usuario']] = '<usuário indefinido>'\n",
    "\n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-string",
   "metadata": {},
   "source": [
    "### DIM situacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "previous-third",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.879890Z",
     "start_time": "2021-10-01T23:04:35.876194Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_Situacao():\n",
    "\n",
    "    print('dim_Situacao - iniciando montagem... ', end='')\n",
    "\n",
    "    dfx = pd.DataFrame({'PK_dim_Situacao': [1, 2, 3],\n",
    "                        'Situacao':        ['em Andamento', 'Encerrada', 'Cancelada']}\n",
    "                        )\n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "\n",
    "    return dfx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-click",
   "metadata": {},
   "source": [
    "### DIM SLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "reported-starter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.886173Z",
     "start_time": "2021-10-01T23:04:35.881978Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_SLA():\n",
    "\n",
    "    print('dim_SLA - iniciando montagem... ', end='')\n",
    "\n",
    "    dfx = pd.DataFrame({'PK_dim_SLA': [1, 2, 3],\n",
    "                        'DescSLA':    ['dentro do prazo', 'perto do prazo', 'fora do prazo'],\n",
    "                        'Cor':        ['verde','amarelo' ,'vermelho']}\n",
    "                    )\n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-syndicate",
   "metadata": {},
   "source": [
    "### DIM endereco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "broke-degree",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.899663Z",
     "start_time": "2021-10-01T23:04:35.888904Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_Endereco(dfe):\n",
    "\n",
    "    print('dim_Endereco - iniciando montagem... ', end='')\n",
    "\n",
    "    dc = ['state', 'city', 'neighborhood', 'zipcode', 'street']\n",
    "    df = dfe.loc[dfe['atributo'].isin(dc), ['atributo','valor', 'protocolo']].copy()\n",
    "# ===================================================    \n",
    "    # faz pivot da coluna atributo\n",
    "    df = df.pivot(columns='atributo', values='valor', index='protocolo').reset_index()\n",
    "    df.columns.name = None\n",
    "\n",
    "    # completa dataset com campos obrigatórios para composição de endereço \n",
    "    for i in dc:\n",
    "        if i not in df.columns:\n",
    "            df[i] = ''\n",
    "    \n",
    "    # renomeia colunas\n",
    "    dic_renome = {'protocolo': 'Solicitacao', \n",
    "                  'zipcode': 'CEP', \n",
    "                  'street': 'Endereco', \n",
    "                  'neighborhood': 'Bairro', \n",
    "                  'city': 'Cidade', \n",
    "                  'state': 'UF'}\n",
    "    df.rename(columns=dic_renome, inplace = True)\n",
    "    \n",
    "    # substitui a coluna Endereco por vazio quando '-' ou null\n",
    "    df.loc[(df['Endereco'] == '-') | (df['Endereco'].isnull()), ['Endereco']] = ''\n",
    "    df.loc[(df['Bairro'] == '-') | (df['Bairro'].isnull()), ['Bairro']] = ''\n",
    "\n",
    "    # cria a coluna Endereco Completo\n",
    "    df['EnderecoCompleto'] = df['Cidade'] + ', ' + df['UF'] + ', Brasil' \n",
    "\n",
    "# ===================================================    \n",
    "\n",
    "    dict_uf = {'AC': 'Acre', 'AL': 'Alagoas', 'AP': 'Amapá', 'AM': 'Amazonas', 'BA': 'Bahia',\n",
    "               'CE': 'Ceará', 'ES': 'Espírito Santo', 'GO': 'Goiás', 'MA': 'Maranhão', 'MT': 'Mato Grosso',\n",
    "               'MS': 'Mato Grosso do Sul', 'MG': 'Minas Gerais', 'PA': 'Pará', 'PB': 'Paraíba',\n",
    "               'PR': 'Paraná', 'PE': 'Pernambuco', 'PI': 'Piauí', 'RJ': 'Rio de Janeiro',\n",
    "               'RN': 'Rio Grande do Norte', 'RS': 'Rio Grande do Sul', 'RO': 'Rondônia', 'RR': 'Roraima',\n",
    "               'SC': 'Santa Catarina', 'SP': 'São Paulo', 'SE': 'Sergipe', 'TO': 'Tocantins', 'DF': 'Distrito Federal',\n",
    "               '<nd>': '<sem UF>'\n",
    "    }\n",
    "\n",
    "    # acrescenta o nome do estado ao dataframe\n",
    "    dfx = df.copy()\n",
    "    dfx['NomeUF'] = dfx['UF'].apply(lambda x: dict_uf.get(x, None))\n",
    "    dfx.rename(columns={'Solicitacao': 'PK_dim_Endereco'}, inplace = True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['CEP'].isna(), ['CEP']] = ''\n",
    "\n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-coordination",
   "metadata": {},
   "source": [
    "### FAT tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "violent-barbados",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.905736Z",
     "start_time": "2021-10-01T23:04:35.902149Z"
    }
   },
   "outputs": [],
   "source": [
    "# função retorna a situação da solicitação de acordo com as coilunas cancelado e encerrado\n",
    "def RetSit(Enc, Canc):\n",
    "    rt = 0\n",
    "    if Canc == 1:\n",
    "        rt = 3\n",
    "    elif Enc == 1:\n",
    "        rt = 2\n",
    "    else:\n",
    "        rt = 1\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "valued-purchase",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.915301Z",
     "start_time": "2021-10-01T23:04:35.907912Z"
    }
   },
   "outputs": [],
   "source": [
    "def CalculaPrazoSolicitacao(df):\n",
    "    # guarda a data atual\n",
    "    AGORA = pd.Timestamp('today')\n",
    "\n",
    "    # cria uma coluna temporária para calcular o prazo\n",
    "    df['tmp_DataHora_Conclusao'] = df['DataHora_Conclusao']\n",
    "\n",
    "    # na coluna temporária, se não houver data de conclusão colocar a datahora atual\n",
    "    df.loc[df['tmp_DataHora_Conclusao'].isna(), 'tmp_DataHora_Conclusao'] = AGORA\n",
    "\n",
    "    # calcula o prazo da tarefa em dias com decimal\n",
    "    df['prazo_task'] = (df['tmp_DataHora_Conclusao'] - df['DataHora_Criacao']) / pd.to_timedelta(1, unit='D')\n",
    "\n",
    "    # cria um dataframe temporário com a soma de prazos de uma solicitação\n",
    "    dfx = df[['FK_dim_Solicitacoes', 'prazo_task']].groupby('FK_dim_Solicitacoes').sum().reset_index()\n",
    "\n",
    "    # cria uma coluna de soma de prazo da respectiva solicitação\n",
    "    df = df.merge(dfx, how='left', on='FK_dim_Solicitacoes')\n",
    "    \n",
    "    # cria um dataframe temporário com a data da solicitação (menor data entre as tasks)\n",
    "    dfx = df[['FK_dim_Solicitacoes', 'DataHora_Criacao']].groupby('FK_dim_Solicitacoes').min().reset_index()\n",
    "    dfx.columns = ['FK_dim_Solicitacoes', 'DataHora_Solicitacao']\n",
    "    \n",
    "    # cria uma coluna com a data da solicitação\n",
    "    df = df.merge(dfx, how='left', on='FK_dim_Solicitacoes')\n",
    "    \n",
    "    df['DataSolicitacao'] = pd.to_datetime(df['DataHora_Solicitacao']).dt.date\n",
    "    df['HoraSolicitacao'] = pd.to_datetime(df['DataHora_Solicitacao']).dt.time    \n",
    "\n",
    "    # exclui a coluna GrupoResponsavel de df_tasks que será a futura tabela fato\n",
    "    df.drop(['tmp_DataHora_Conclusao', 'DataHora_Solicitacao'], axis=1, inplace=True)\n",
    "    df.rename(columns={'prazo_task_y': 'PrazoSolicitacao', 'prazo_task_x': 'PrazoTask'}, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-religion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.930768Z",
     "start_time": "2021-10-01T23:04:35.918138Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_Tasks(df, dfr):\n",
    "\n",
    "    print('fat_Tasks - iniciando montagem... ', end='')\n",
    "\n",
    "    dfx = df.copy()\n",
    "    \n",
    "    # faz um merge com o dataset de ratings\n",
    "    dfx = df.merge(dfr, left_on='Protocolo', right_on='Solicitacao', how='left')\n",
    "    dfx.drop(['Solicitacao'], axis=1, inplace=True)\n",
    "\n",
    "    # define o código de situação da solicitação\n",
    "    df_aux = dfx[['Protocolo', 'ProcessoEncerrado', 'ProcessoCancelado']].drop_duplicates()\n",
    "\n",
    "    # substui valores de colunas\n",
    "    troca = {'false': 0, 'true': 1}\n",
    "    df_aux['ProcessoEncerrado'] = df_aux['ProcessoEncerrado'].map(troca)\n",
    "    df_aux['ProcessoCancelado'] = df_aux['ProcessoCancelado'].map(troca)\n",
    "\n",
    "    # agrega as ações para um registro por protocolo\n",
    "    df_aux = df_aux.groupby('Protocolo').agg({'ProcessoEncerrado': 'max', 'ProcessoCancelado': 'max'}).reset_index()\n",
    "\n",
    "    # retorna a situção da solicitação: 1-em andamento, 2-encerrado, 3-cancelado\n",
    "    df_aux['Situacao'] = df_aux.apply(lambda x: RetSit(x['ProcessoEncerrado'], x['ProcessoCancelado']), axis=1)\n",
    "    df_aux.drop(['ProcessoEncerrado', 'ProcessoCancelado'], axis=1, inplace=True)\n",
    "\n",
    "    # faz merge com o dataset de situação da solicitação\n",
    "    dfx = dfx.merge(df_aux, left_on='Protocolo', right_on='Protocolo', how='left')\n",
    "    dfx.drop(['ProcessoEncerrado', 'ProcessoCancelado'], axis=1, inplace=True)\n",
    "\n",
    "    # coloca -1 para as solicitações que não possuem avaliação\n",
    "    dfx.loc[dfx['NotaAvaliacao'].isna(), 'NotaAvaliacao'] = -1\n",
    "    dfx['NotaAvaliacao'] = dfx['NotaAvaliacao'].astype('int32')\n",
    "\n",
    "    # renomeia as colunas\n",
    "    col_ren = {'Protocolo': 'FK_dim_Solicitacoes', 'Situacao': 'FK_dim_Situacao'}\n",
    "    dfx.rename(columns=col_ren, inplace = True)\n",
    "\n",
    "    # trata a coluna Prazo\n",
    "    \n",
    "    dfx.loc[dfx['Prazo'].isna(), 'Prazo'] = '0'\n",
    "#     dfx.loc[(dfx['Prazo'] == '') | (dfx['Prazo'] == 'null'), 'Prazo'] = '0'\n",
    "    dfx['Prazo'] = dfx['Prazo'].astype('int64')\n",
    "\n",
    "    dfx['DataCriacao'] = pd.to_datetime(dfx['DataHora_Criacao']).dt.date\n",
    "    dfx['HoraCriacao'] = pd.to_datetime(dfx['DataHora_Criacao']).dt.time\n",
    "\n",
    "    dfx['DataConclusao'] = pd.to_datetime(dfx['DataHora_Conclusao']).dt.date\n",
    "    dfx['HoraConclusao'] = pd.to_datetime(dfx['DataHora_Conclusao']).dt.time\n",
    "\n",
    "    dfx = CalculaPrazoSolicitacao(dfx)\n",
    "\n",
    "    cols_ordem = ['FK_dim_Solicitacoes', 'Prazo', \n",
    "                  'DataCriacao', 'HoraCriacao', 'DataHora_Criacao',\n",
    "                  'DataConclusao', 'HoraConclusao', 'DataHora_Conclusao',\n",
    "                  'NotaAvaliacao', 'MotivoAvaliacao', \n",
    "                  'DataAvaliacao', 'HoraAvaliacao', \n",
    "                  'DataSolicitacao', 'HoraSolicitacao',\n",
    "                  'PrazoSolicitacao', 'PrazoTask',\n",
    "                  'FK_dim_Entidades', 'FK_dim_Servicos', 'FK_dim_Usuarios', 'FK_dim_GruposUsuarios', \n",
    "                  'FK_dim_Acoes', 'FK_dim_StatusExt', 'FK_dim_CategoriasServicos', 'FK_dim_GrupoResponsavel', \n",
    "                  'FK_dim_MotivosCanc', 'FK_dim_Encaminhamento', 'FK_dim_Situacao']\n",
    "    dfx = dfx[cols_ordem]\n",
    "\n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "\n",
    "    return dfx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-sandwich",
   "metadata": {},
   "source": [
    "### <font color='green'>META BI - Constantes</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-gregory",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.940350Z",
     "start_time": "2021-10-01T23:04:35.933282Z"
    }
   },
   "outputs": [],
   "source": [
    "# tabela de decisão de combinação Dimensões e Métricas\n",
    "mtx =  [('STRING_DIM',  'INTEGER_MET', 'N', 'N', 'S', 'S'),\n",
    "        ('STRING_DIM',  'DOUBLE_MET',  'N', 'N', 'S', 'S'),\n",
    "        ('STRING_DIM',  'NULL_MET',    'S', 'S', 'N', 'N'),\n",
    "\n",
    "        ('BOOLEAN_DIM', 'INTEGER_MET', 'S', 'N', 'N', 'N'),\n",
    "        ('BOOLEAN_DIM', 'DOUBLE_MET',  'S', 'N', 'N', 'N'),\n",
    "        ('BOOLEAN_DIM', 'NULL_MET',    'S', 'N', 'N', 'N'),\n",
    "\n",
    "        ('INTEGER_DIM', 'INTEGER_MET', 'S', 'S', 'S', 'S'),\n",
    "        ('INTEGER_DIM', 'DOUBLE_MET',  'N', 'N', 'N', 'N'),\n",
    "        ('INTEGER_DIM', 'NULL_MET',    'S', 'S', 'N', 'N'),\n",
    "\n",
    "        ('DOUBLE_MET',  'INTEGER_MET', 'N', 'N', 'N', 'N'),\n",
    "        ('DOUBLE_MET',  'DOUBLE_MET',  'N', 'N', 'S', 'S'),\n",
    "        ('DOUBLE_MET',  'NULL_MET',    'N', 'N', 'N', 'N'),\n",
    "\n",
    "        ('ID_DIM',      'INTEGER_MET', 'N', 'N', 'S', 'S'),\n",
    "        ('ID_DIM',      'DOUBLE_MET',  'N', 'N', 'S', 'S'),\n",
    "        ('ID_DIM',      'NULL_MET',    'S', 'S', 'N', 'N')\n",
    "        ]\n",
    "# colunas da tabela de decisão de combinação Dimensões e Métricas\n",
    "cols_mtx = ['t_dim', 't_met', 'Count', 'Distinct Count', 'Sum', 'Average']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-scanner",
   "metadata": {},
   "source": [
    "### <font color='green'>META BI - Tabelas Auxiliares</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-progress",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.947451Z",
     "start_time": "2021-10-01T23:04:35.943166Z"
    }
   },
   "outputs": [],
   "source": [
    "def GeraCodDimMet(d, m):\n",
    "    d_dim = {'STRING_DIM': 10, 'BOOLEAN_DIM': 20, 'INTEGER_DIM' :30, 'DOUBLE_DIM': 40, 'ID_DIM': 50}\n",
    "    d_met = {'INTEGER_MET': 1, 'DOUBLE_MET': 2, 'NULL_MET': 3}\n",
    "\n",
    "    v_dim = d_dim[d] if d in d_dim else 0\n",
    "    v_met = d_met[m] if m in d_met else 0\n",
    "    \n",
    "    return v_dim + v_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-batman",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.959130Z",
     "start_time": "2021-10-01T23:04:35.955301Z"
    }
   },
   "outputs": [],
   "source": [
    "def Enumerico(s, tipo):\n",
    "    try:\n",
    "        v = np.float64(s) if tipo == 'DOUBLE' else np.int64(s)\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-mambo",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.965196Z",
     "start_time": "2021-10-01T23:04:35.961324Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_aux_entidade(df):\n",
    "    # recebe df=tasks\n",
    "\n",
    "    print('Z_aux_entidade - iniciando montagem... ', end='')\n",
    "\n",
    "    df_ret = df[['Protocolo', 'Entidade']].drop_duplicates()\n",
    "    df_ret.columns = ['protocolo', 'entidade']\n",
    "    \n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-gates",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.973972Z",
     "start_time": "2021-10-01T23:04:35.967604Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_aux_BASE(df, df2):\n",
    "    # recebe df=form, df2=df_aux_entidade\n",
    "\n",
    "    print('Z_aux_BASE - iniciando montagem... ', end='')\n",
    "\n",
    "    df_ret = df.copy()\n",
    "    \n",
    "    # transforma as strings atributo e campo relacionado em lower case\n",
    "    df_ret[['atributo', 'campo_relacionado']] = df_ret[['atributo', 'campo_relacionado']].apply(lambda x: x.str.lower())\n",
    "\n",
    "    # faz um merge da dimensão df_aux_BASE com a df_auxentidade para adicionar a coluna entidade\n",
    "    # se não existir o protocolo na tasks não considera a linha -- isso é um erro de integridade da base de dados\n",
    "    df_ret = df_ret.merge(df2, on='protocolo')    \n",
    "    \n",
    "    # cria 3 novas colunas vazias\n",
    "    df_ret['MET_atributo'] = np.nan\n",
    "    df_ret['MET_valor']    = np.nan\n",
    "    df_ret['MET_tipo']     = np.nan\n",
    "\n",
    "    # cria uma coluna como PK\n",
    "    df_ret['PK'] = df_ret['entidade'] + '_' + df_ret['servico'] + '_' + \\\n",
    "                        df_ret['atributo'] + '_' + df_ret['protocolo']\n",
    "\n",
    "    # cria uma coluna com o tipo de registro\n",
    "    df_ret['TIPO_REG'] = 'BASE'\n",
    "    \n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "    \n",
    "    return df_ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-victoria",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.985580Z",
     "start_time": "2021-10-01T23:04:35.976621Z"
    }
   },
   "outputs": [],
   "source": [
    "# traramento do tipo INTEGER ou DOUBLE\n",
    "def Z_aux_MET(df):\n",
    "    # recebe df=df_aux_BASE\n",
    "\n",
    "    print('Z_aux_MET e Z_aux_MET_Err - iniciando montagem... ', end='')\n",
    "\n",
    "    # dataset de métricas - usa somente tipo \"INTEGER\" ou \"DOUBLE\"\n",
    "    cols = ['atributo', 'nome', 'valor', 'protocolo', 'servico', 'tipo', 'campo_relacionado', 'entidade']\n",
    "    df_ret = df.loc[(df['tipo'] == 'INTEGER') | (df['tipo'] == 'DOUBLE'), cols].copy()\n",
    "\n",
    "    # os valores estão com formato: ponto para separador de milhar e vírgula para fração\n",
    "    # tira os pontos separador de milhar e substitui vírgula por ponto\n",
    "    df_ret['valor'] = df_ret['valor'].apply(lambda x: x.replace('.', '').replace(',', '.'))\n",
    "\n",
    "    # alimenta uma coluna indicadora que mostra se o valor corresponde ou não ao tipo\n",
    "    df_ret['numerico'] = df_ret.apply(lambda x: Enumerico(x['valor'], x['tipo']), axis=1)\n",
    "\n",
    "    # cria um dataset de linhas cuja o valor não corresponde ao tipo\n",
    "    df_ret_Err = df_ret[df_ret['numerico'] == False].drop('numerico', axis=1)\n",
    "\n",
    "    # cria um dataset de linhas de métricas\n",
    "    df_ret = df_ret[~df_ret['numerico'] == False].drop('numerico', axis=1)\n",
    "\n",
    "    # identifica que esse é um dataset de métricas\n",
    "    df_ret['TIPO_REG'] = 'MET'\n",
    "\n",
    "    # exclui linhas que não seja uma métrica explícita\n",
    "    df_ret = df_ret[~(df_ret['campo_relacionado'] == '')]\n",
    "\n",
    "    # cria a coluna FK para relacionar com o dataset BASE\n",
    "    df_ret['FK'] = df_ret['entidade'] + '_' + df_ret['servico'] + '_' + \\\n",
    "                   df_ret['campo_relacionado'] + '_' + df_ret['protocolo']\n",
    "\n",
    "    # troca o nome de algumas colunas\n",
    "    dc = {'atributo': 'MET_atributo', 'campo_relacionado': 'atributo', 'valor': 'MET_valor', 'tipo': 'MET_tipo'}\n",
    "    df_ret.rename(columns=dc, inplace = True)\n",
    "\n",
    "    # faz um merge com o dataset BASE \n",
    "    df_ret = df_ret.merge(df[['PK', 'valor', 'tipo']], left_on='FK', right_on='PK')\n",
    "    df_ret.drop(['PK', 'FK'], axis=1, inplace=True)\n",
    "    \n",
    "    print('%d e %d linhas OK.' %(df_ret.shape[0], df_ret_Err.shape[0]))\n",
    "\n",
    "    return df_ret, df_ret_Err\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-brand",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:35.993481Z",
     "start_time": "2021-10-01T23:04:35.987843Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_aux_DIM(df, df_m):\n",
    "# recebe df=df_aux_BASE, df_m=df_aux_MET\n",
    "\n",
    "    print('Z_aux_DIM - iniciando montagem... ', end='')\n",
    "\n",
    "    # escolee as colunas que farão a cancatenação de dataframes\n",
    "    cols = ['atributo', 'nome', 'valor', 'protocolo', 'servico', 'tipo', \n",
    "            'entidade', 'MET_atributo', 'MET_valor', 'MET_tipo', 'TIPO_REG']\n",
    "    df_ret = df[cols].copy()\n",
    "    df_ret = pd.concat([df_ret, df_m])\n",
    "    \n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-feedback",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:36.000183Z",
     "start_time": "2021-10-01T23:04:35.996349Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_aux_decisao():\n",
    "\n",
    "    print('Z_aux_decisao - iniciando montagem... ', end='')\n",
    "\n",
    "    df_ret = pd.DataFrame(mtx, columns=cols_mtx)\n",
    "    df_ret['Cod_Decisao'] = df_ret.apply(lambda x: GeraCodDimMet(x['t_dim'], x['t_met']), axis=1)\n",
    "    \n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "    \n",
    "    return df_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-religious",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:36.007276Z",
     "start_time": "2021-10-01T23:04:36.002558Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_aux_DataSolicitacao(df):\n",
    "    # recebe df=tasks\n",
    "\n",
    "    print('Z_aux_DataSolicitacao - iniciando montagem... ', end='')\n",
    "\n",
    "    df_tmp = df[['Protocolo', 'DataHora_Criacao']].copy()\n",
    "    # cria uma coluna só de datas\n",
    "    df_tmp['data_criacao'] = pd.to_datetime(df_tmp['DataHora_Criacao']).dt.date\n",
    "\n",
    "    # pega a menor data de cada número de protocolo\n",
    "    df_ret = df_tmp[['Protocolo', 'data_criacao']].groupby('Protocolo').agg('min').reset_index()\n",
    "    df_ret.columns = ['protocolo', 'data_criacao']\n",
    "    \n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-electricity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:36.015853Z",
     "start_time": "2021-10-01T23:04:36.010229Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_aux_pto(df):\n",
    "    # recebe df=df_aux_dec\n",
    "\n",
    "    print('Z_aux_pto - iniciando montagem... ', end='')\n",
    "\n",
    "    # faz uma cópia da tabela de decisão com as colunas necessárias\n",
    "    cols = ['Cod_Decisao'] + cols_mtx[2:]\n",
    "    df_ret = df[cols].copy()\n",
    "\n",
    "    # faz o unpivot\n",
    "    df_ret = pd.melt(df_ret, id_vars=['Cod_Decisao'], \n",
    "                     value_vars=['Count', 'Distinct Count', 'Sum', 'Average'],\n",
    "                     var_name='Atributo')\n",
    "    df_ret = df_ret[df_ret['value'] == 'S'].drop('value', axis=1)\n",
    "\n",
    "    # trasnforma a coluna Atributo em uma coluna do tipo \"category'\n",
    "    df_ret['Atributo'] = df_ret['Atributo'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Operação\n",
    "    df_ret['FK_ZN_dim_OPERACAO'] = df_ret['Atributo'].cat.codes.astype('int64') + 1\n",
    "    \n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-cambodia",
   "metadata": {},
   "source": [
    "### <font color='green'>META BI - Tabelas Dim, Fat e Pto</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-frequency",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:36.022529Z",
     "start_time": "2021-10-01T23:04:36.018570Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_pto_Decisao_Operacao(df):\n",
    "    # recebe df=df_Z_aux_pto\n",
    "\n",
    "    print('Z_pto_decisao_operacao - iniciando montagem... ', end='')\n",
    "\n",
    "    df_ret = df.copy()\n",
    "    \n",
    "    # cria a tabela df_pto_decisao_operacao necessária ao modelo dimensional\n",
    "    df_ret = df_ret[['Cod_Decisao', 'FK_ZN_dim_OPERACAO']]\n",
    "    df_ret.columns = ['FK_dim_DECISAO', 'FK_ZN_dim_OPERACAO']\n",
    "    \n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-leather",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:36.029493Z",
     "start_time": "2021-10-01T23:04:36.024661Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_dim_DIM(df):\n",
    "    # recebe df=df_aux_DIM\n",
    "\n",
    "    print('Z_dim_DIM - iniciando montagem... ', end='')\n",
    "\n",
    "    df_ret = df.copy()\n",
    "\n",
    "    df_ret = df_ret[['atributo', 'nome']].drop_duplicates().reset_index(drop=True).reset_index()\n",
    "    df_ret.columns = ['PK_Z_dim_DIM', 'dimensao', 'nome']\n",
    "    df_ret['PK_Z_dim_DIM'] += 1\n",
    "    \n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-passion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:36.036255Z",
     "start_time": "2021-10-01T23:04:36.032068Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_dim_SERVICO(df):    \n",
    "    # recebe df=df_aux_DIM\n",
    "\n",
    "    print('Z_dim_SERVICO - iniciando montagem... ', end='')\n",
    "\n",
    "    df_ret = df.copy()\n",
    "\n",
    "    df_ret = df_ret['servico'].drop_duplicates().reset_index(drop=True).reset_index()\n",
    "    df_ret.columns = ['PK_Z_dim_SERVICO', 'servico']\n",
    "    df_ret['PK_Z_dim_SERVICO'] += 1\n",
    "    \n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-arlington",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:36.043920Z",
     "start_time": "2021-10-01T23:04:36.038366Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_dim_MET(df):    \n",
    "    # recebe df=df_aux_DIM\n",
    "\n",
    "    print('Z_dim_MET - iniciando montagem... ', end='')\n",
    "\n",
    "    df_ret = df.copy()\n",
    "\n",
    "    df_ret = df_ret.loc[~df_ret['MET_atributo'].isna(), 'MET_atributo'].drop_duplicates()\n",
    "    df_ret = df_ret.reset_index(drop=True).reset_index()\n",
    "    df_ret.columns = ['PK_Z_dim_MET', 'metrica']\n",
    "    df_ret['PK_Z_dim_MET'] += 1\n",
    "\n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-vegetable",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:36.050332Z",
     "start_time": "2021-10-01T23:04:36.046214Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_dim_OPERACAO(df):    \n",
    "    # recebe df=df_aux_pto\n",
    "\n",
    "    print('Z_dim_OPERACAO - iniciando montagem... ', end='')\n",
    "\n",
    "    df_ret = df.copy()\n",
    "\n",
    "    df_ret = df_ret[['FK_ZN_dim_OPERACAO', 'Atributo']].drop_duplicates()\n",
    "    df_ret.columns = ['PK_Z_dim_OPERACAO', 'operacao']\n",
    "\n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-custom",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:36.057692Z",
     "start_time": "2021-10-01T23:04:36.053308Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_dim_DECISAO(df):    \n",
    "    # recebe df=df_aux_dec\n",
    "\n",
    "    print('Z_dim_DECISAO - iniciando montagem... ', end='')\n",
    "\n",
    "    df_ret = df.copy()\n",
    "\n",
    "    df_ret = df_ret[['Cod_Decisao', 't_dim', 't_met']].copy().rename(columns={'Cod_Decisao': 'PK_Z_dim_DECISAO'})\n",
    "\n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-patent",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:36.073802Z",
     "start_time": "2021-10-01T23:04:36.060628Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_fat_META(df, df_dim, df_ser, df_met, df_dts):    \n",
    "    # recebe df=df_Z_dim_DIM, df_ser=df_Z_dim_SERVICO, df_met= df_Z_dim_MET, df_dts=df_Z_aux_DataSolicitacao\n",
    "\n",
    "    print('Z_fat_META - iniciando montagem... ', end='')\n",
    "\n",
    "    # copia o df auxiliar\n",
    "    df_ret = df.copy()\n",
    "\n",
    "    # faz merge com dim_DIM\n",
    "    df_ret = df_ret.merge(df_dim, \n",
    "                          how='left', \n",
    "                          left_on=['atributo', 'nome'], \n",
    "                          right_on=['dimensao', 'nome']).drop(['atributo', 'nome', 'dimensao'], axis=1)\n",
    "    # faz merge com dim_SERVICO\n",
    "    df_ret = df_ret.merge(df_ser, \n",
    "                          how='left', \n",
    "                          left_on='servico', \n",
    "                          right_on='servico').drop('servico', axis=1)\n",
    "    # faz merge com dim_MET\n",
    "    df_ret = df_ret.merge(df_met, \n",
    "                          how='left', \n",
    "                          left_on='MET_atributo', \n",
    "                          right_on='metrica').drop(['MET_atributo', 'metrica'], axis=1)\n",
    "    # faz merge com Z_aux_DataSolicitacao\n",
    "    df_ret = df_ret.merge(df_dts, \n",
    "                          how='left', \n",
    "                          on='protocolo') #.drop(['MET_atributo', 'metrica'], axis=1)\n",
    "\n",
    "    # cria a coluna indicadora de NULL_MET\n",
    "    df_ret['reg_nativo'] = df_ret['MET_tipo'].apply(lambda x: 1 if pd.isna(x) else 0)\n",
    "\n",
    "    # passa as colunas de tipo para a string indicadora de NULL\n",
    "    df_ret.loc[df_ret['MET_tipo'].isna(), 'MET_tipo'] = 'NULL_MET'\n",
    "    df_ret.loc[df_ret['tipo'].isna(), 'tipo'] = 'NULL_DIM'\n",
    "\n",
    "\n",
    "    # substitui valores da coluna tipo\n",
    "    subs_dim = {'STRING':'STRING_DIM', \n",
    "                'INTEGER': 'INTEGER_DIM', \n",
    "                'BOOLEAN':'BOOLEAN_DIM', \n",
    "                'DOUBLE': 'DOUBLE_DIM',\n",
    "                'ID':'ID_DIM'}\n",
    "    subs_met = {'INTEGER': 'INTEGER_MET', \n",
    "                'DOUBLE': 'DOUBLE_MET'}\n",
    "\n",
    "    df_ret['tipo'].replace(subs_dim, inplace=True)\n",
    "    df_ret['MET_tipo'].replace(subs_met, inplace=True)\n",
    "\n",
    "    # acha o código para a tabela de decisão da dupla de tipos como FK para a PK de \"df_aux_dec\"\n",
    "    df_ret['FK_Z_dim_DECISAO'] = df_ret.apply(lambda x: GeraCodDimMet(x['tipo'], x['MET_tipo']), axis=1)\n",
    "\n",
    "    # exclui colunas já usadas\n",
    "    df_ret.drop(['tipo', 'MET_tipo'], axis=1, inplace=True)\n",
    "\n",
    "    # acerta os nomes das colunas\n",
    "    dc = {'PK_Z_dim_DIM': 'FK_Z_dim_DIM', \n",
    "          'PK_Z_dim_SERVICO': 'FK_Z_dim_SERVICO', \n",
    "          'PK_Z_dim_MET': 'FK_Z_dim_MET', \n",
    "          'data_criacao': 'FK_Z_Data_Criacao'}\n",
    "    df_ret.rename(columns=dc, inplace = True)\n",
    "\n",
    "    # acerta os tipos das colunas\n",
    "    df_ret['FK_Z_dim_MET'] = df_ret['FK_Z_dim_MET'].astype('Int64')\n",
    "    df_ret['MET_valor'] = df_ret['MET_valor'].astype('float64')\n",
    "    df_ret['FK_Z_Data_Criacao'] = df_ret['FK_Z_Data_Criacao'].apply(pd.to_datetime)\n",
    "\n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-break",
   "metadata": {},
   "source": [
    "# <font color='red'>4.0 LOAD</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-appointment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:36.083574Z",
     "start_time": "2021-10-01T23:04:36.077097Z"
    }
   },
   "outputs": [],
   "source": [
    "def Regras():\n",
    "    tp_dfs = (\n",
    "        (df_dim_Acao,               'dim_acoes',                 'replace'),               \n",
    "        (df_dim_Categoria,          'dim_categorias_servicos',   'replace'), \n",
    "        (df_dim_Encaminhamento,     'dim_encaminhamento',        'replace'),      \n",
    "        (df_dim_Entidade,           'dim_entidades',             'replace'),           \n",
    "        (df_dim_GrupoResponsavel,   'dim_grupo_responsavel',     'replace'),   \n",
    "        (df_dim_Grupo,              'dim_grupos_usuarios',       'replace'),     \n",
    "        (df_dim_MotivoCanc,         'dim_motivos_canc',          'replace'),        \n",
    "        (df_dim_Servicos,           'dim_servicos',              'replace'),            \n",
    "        (df_dim_StatusExt,          'dim_status_ext',            'replace'),          \n",
    "        (df_dim_Usuarios,           'dim_usuarios',              'replace'),            \n",
    "        (df_dim_Situacao,           'dim_situacao',              'replace'),            \n",
    "        (df_dim_SLA,                'dim_sla',                   'replace'),                 \n",
    "        (df_dim_Endereco,           'dim_endereco',              'replace'),            \n",
    "        (df_fat_tasks,              'fat_tasks',                 'replace'),               \n",
    "        (df_Z_pto_decisao_operacao, 'z_pto_decisao_operacao',    'replace'),  \n",
    "        (df_Z_dim_DIM,              'z_dim_dim',                 'replace'),  \n",
    "        (df_Z_dim_SERVICO,          'z_dim_servico',             'replace'),  \n",
    "        (df_Z_dim_MET,              'z_dim_met',                 'replace'),  \n",
    "        (df_Z_dim_OPERACAO,         'z_dim_operacao',            'replace'),  \n",
    "        (df_Z_dim_DECISAO,          'z_dim_decisao',             'replace'),  \n",
    "        (df_Z_fat_META,             'z_fat_meta',                'replace'),\n",
    "        (df_ts,                     'ctl_carga',                 'append')\n",
    "    )\n",
    "    return tp_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-cleanup",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:36.092774Z",
     "start_time": "2021-10-01T23:04:36.086177Z"
    }
   },
   "outputs": [],
   "source": [
    "def Load_BD(strAut):\n",
    "\n",
    "    # import the module\n",
    "    # create sqlalchemy engine\n",
    "    \n",
    "    bd_load = \"mysql+pymysql://{user}:{pw}@{host}/{db}\"\n",
    "    print('LoadBD - carregando tabelas ... ')\n",
    "\n",
    "    SQLengine = create_engine(bd_load\n",
    "                           .format(user = strAut['My_user'],\n",
    "                                   pw   = strAut['My_pw'],\n",
    "                                   host = strAut['My_host'],\n",
    "                                   db   = strAut['My_db']),\n",
    "                                   pool_recycle=3600)\n",
    "    dbConn = SQLengine.connect()\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for i in Regras():\n",
    "            print('    carregando tabela {} ... '.format(i[1]), end='')\n",
    "            i[0].to_sql(i[1], con=dbConn, if_exists=i[2], index=False, chunksize = 10000)\n",
    "            print('OK {0: .2f}'.format((time.time() - start_time)))\n",
    "        print('LoadBD - carga de tabelas OK')\n",
    "        \n",
    "    except ValueError as vx:\n",
    "        print('ERROR -', vx)\n",
    "\n",
    "    except Exception as ex: \n",
    "        print('EXCEPTION -', ex)\n",
    "\n",
    "    else:\n",
    "        print('Tabelas criadas com sucesso');  \n",
    "\n",
    "    finally:\n",
    "        dbConn.close()\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9fa4f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:36.101016Z",
     "start_time": "2021-10-01T23:04:36.095451Z"
    }
   },
   "outputs": [],
   "source": [
    "def New_Load_BD():\n",
    "\n",
    "    # import the module\n",
    "    print('LoadBD - carregando tabelas ... ')\n",
    "\n",
    "    dbConn = ConectaBD(ambientes[amb][2], 1)\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for i in Regras():\n",
    "            print('    carregando tabela {} ... '.format(i[1]), end='')\n",
    "            i[0].to_sql(i[1], con=dbConn, if_exists=i[2], index=False, chunksize = 10000)\n",
    "            print('OK {0: .2f}'.format((time.time() - start_time)))\n",
    "        print('LoadBD - carga de tabelas OK')\n",
    "        \n",
    "    except ValueError as vx:\n",
    "        print('ERROR -', vx)\n",
    "\n",
    "    except Exception as ex: \n",
    "        print('EXCEPTION -', ex)\n",
    "\n",
    "    else:\n",
    "        print('Tabelas criadas com sucesso');  \n",
    "\n",
    "    finally:\n",
    "        dbConn.close()\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-asian",
   "metadata": {},
   "source": [
    "# <font color='red'>0.0 MAIN</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-province",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:04:36.106090Z",
     "start_time": "2021-10-01T23:04:36.102725Z"
    }
   },
   "outputs": [],
   "source": [
    "def gd_timestamp():\n",
    "    lst = {'DataHora': [datetime.today()], 'Versao': [ETL_VERSAO]}\n",
    "    df = pd.DataFrame(lst)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-campbell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T23:31:49.691547Z",
     "start_time": "2021-10-01T23:12:23.502887Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    print(f'Iniciando ETL de {ambientes[amb][0]}')\n",
    "    \n",
    "    # EXTRACT\n",
    "    df_tasks, df_sla, df_form, df_rating = LeFontes()\n",
    "    \n",
    "    DF_TESTE = df_sla.copy()\n",
    "    \n",
    "    # TRANSFORM\n",
    "    # trata os datasets\n",
    "    df_sla    = trSLA(df_sla)\n",
    "    df_form   = trForm(df_form)\n",
    "    df_rating = trRating(df_rating)\n",
    "    df_tasks  = trTasks(df_tasks)\n",
    "    \n",
    "    dft = df_tasks.copy()\n",
    "    dff = df_form.copy()\n",
    "    \n",
    "    # monta as tabelas dimensão\n",
    "    df_dim_Acao              = Mt_dim_Acoes(df_tasks)\n",
    "    df_dim_Categoria         = Mt_dim_CategoriaServico(df_tasks)\n",
    "    df_dim_Encaminhamento    = Mt_dim_Encaminhamento(df_tasks)\n",
    "    df_dim_Entidade          = Mt_dim_Entidade(df_tasks)\n",
    "    df_dim_GrupoResponsavel  = Mt_dim_GrupoResponsavel(df_tasks)\n",
    "    df_dim_Grupo             = Mt_dim_GruposUsuarios(df_tasks)\n",
    "    df_dim_MotivoCanc        = Mt_MotivosCanc(df_tasks)\n",
    "    df_dim_Servicos          = Mt_Servico(df_tasks, df_sla)\n",
    "    df_dim_StatusExt         = Mt_StatusExt(df_tasks)\n",
    "    df_dim_Usuarios          = Mt_Usuarios(df_tasks)\n",
    "    df_dim_Situacao          = Mt_Situacao()\n",
    "    df_dim_SLA               = Mt_SLA()\n",
    "    df_dim_Endereco          = Mt_Endereco(df_form)\n",
    "    \n",
    "    # monta a tabela fato\n",
    "    df_fat_tasks             = Mt_Tasks(df_tasks, df_rating)\n",
    "    \n",
    "    # monta as tabelas auxiliares do META BI\n",
    "    df_Z_aux_entidade              = Z_aux_entidade(dft)\n",
    "    df_Z_aux_BASE                  = Z_aux_BASE(dff, df_Z_aux_entidade)\n",
    "    df_Z_aux_MET, df_Z_aux_MET_Err = Z_aux_MET(df_Z_aux_BASE)\n",
    "    df_Z_aux_DIM                   = Z_aux_DIM(df_Z_aux_BASE, df_Z_aux_MET)\n",
    "    df_Z_aux_decisao               = Z_aux_decisao()\n",
    "    df_Z_aux_DataSolicitacao       = Z_aux_DataSolicitacao(dft)\n",
    "    df_Z_aux_pto                   = Z_aux_pto(df_Z_aux_decisao)\n",
    "    \n",
    "    # monta as tabelas dimensão do META BI\n",
    "    df_Z_pto_decisao_operacao      = Z_pto_Decisao_Operacao(df_Z_aux_pto)\n",
    "    df_Z_dim_DIM                   = Z_dim_DIM(df_Z_aux_DIM)\n",
    "    df_Z_dim_SERVICO               = Z_dim_SERVICO(df_Z_aux_DIM)\n",
    "    df_Z_dim_MET                   = Z_dim_MET(df_Z_aux_DIM)\n",
    "    df_Z_dim_OPERACAO              = Z_dim_OPERACAO(df_Z_aux_pto)\n",
    "    df_Z_dim_DECISAO               = Z_dim_DECISAO(df_Z_aux_decisao)\n",
    "\n",
    "    # monta as tabelas dimensão do META BI\n",
    "    df_Z_fat_META                  = Z_fat_META(df_Z_aux_DIM,\n",
    "                                                df_Z_dim_DIM, \n",
    "                                                df_Z_dim_SERVICO, \n",
    "                                                df_Z_dim_MET, \n",
    "                                                df_Z_aux_DataSolicitacao)\n",
    "    \n",
    "    # monta data e hora e versão da atualização\n",
    "    df_ts = gd_timestamp()\n",
    "\n",
    "    # LOAD\n",
    "    ret = New_Load_BD()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-prototype",
   "metadata": {},
   "source": [
    "# TESTE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
