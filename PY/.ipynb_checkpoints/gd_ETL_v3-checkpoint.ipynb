{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "becoming-coalition",
   "metadata": {},
   "source": [
    "# <font color='red'>1.0 IMPORT</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "collectible-company",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:36:28.881831Z",
     "start_time": "2021-08-16T22:36:19.137746Z"
    }
   },
   "outputs": [],
   "source": [
    "# ENVIRONMENT\n",
    "\n",
    "# !pip install mysql-connector-python\n",
    "# !pip install pymysql\n",
    "# !pip install psycopg2\n",
    "# !pip install sqlalchemy\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import mysql.connector\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# pd.options.display.max_rows = 2000\n",
    "# pd.options.display.width = 120\n",
    "# pd.options.display.max_colwidth = 100\n",
    "\n",
    "ETL_VERSAO = 'etl_v1.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-collar",
   "metadata": {},
   "source": [
    "# <font color='red'>2.0 EXTRACT</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blocked-hanging",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:36:33.197800Z",
     "start_time": "2021-08-16T22:36:33.184173Z"
    }
   },
   "outputs": [],
   "source": [
    "def LeFontes(strAut):\n",
    "\n",
    "    print('Iniciando leitura de dados nativos...', end='')\n",
    "    \n",
    "    sql_form = (\n",
    "    \"SELECT * \"\n",
    "    \"FROM form\"\n",
    "    )\n",
    "    sql_tasks = (\n",
    "    \"SELECT DISTINCT `Protocolo`, `Entidade`, `Serviço`, `Usuário`, `Grupo`, `Data e Hora de conclusão`, \"\n",
    "    \"`Data e Hora de criação`, `Ação`, `Encaminhado para`, `Processo encerrado`, `Processo cancelado`, \"\n",
    "    \"`Motivo de cancelamento`, `Status externo`, `Categoria`, `Grupo responsável`, `Prazo (em segundos)` \"\n",
    "    \"FROM tasks\"\n",
    "    )\n",
    "    sql_sla = (\n",
    "    \"SELECT * \"\n",
    "    \"FROM sla\"\n",
    "    )\n",
    "    sql_rating = (\n",
    "    \"SELECT * \"\n",
    "    \"FROM rating\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        connection = mysql.connector.connect(host     = strAut['My_host'], \n",
    "                                             database = strAut['My_db'], \n",
    "                                             user     = strAut['My_user'], \n",
    "                                             password = strAut['My_pw'])\n",
    "\n",
    "        df_int_tasks =   pd.read_sql(sql_tasks,  con=connection)\n",
    "        df_int_sla =     pd.read_sql(sql_sla,    con=connection)\n",
    "        df_int_form =    pd.read_sql(sql_form,   con=connection)\n",
    "        df_int_rating =  pd.read_sql(sql_rating, con=connection)\n",
    "        \n",
    "        print('OK')\n",
    "\n",
    "    except mysql.connector.Error as error:\n",
    "        print(\"Failed to read record from MySQL table {}\".format(error))\n",
    "\n",
    "    finally:\n",
    "        if (connection.is_connected()):\n",
    "            connection.close()\n",
    "            print(f'tasks:{df_int_tasks.shape[0]} registros lidos em {df_int_tasks.shape[1]} colunas')\n",
    "            print(f'sla: {df_int_sla.shape[0]} registros lidos em {df_int_sla.shape[1]} colunas')\n",
    "            print(f'form: {df_int_form.shape[0]} registros lidos em {df_int_form.shape[1]} colunas')\n",
    "            print(f'rating: {df_int_rating.shape[0]} registros lidos em {df_int_rating.shape[1]} colunas')\n",
    "            print(\"MySQL connection is closed\")\n",
    "            \n",
    "    return df_int_tasks, df_int_sla, df_int_form, df_int_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-river",
   "metadata": {},
   "source": [
    "# <font color='red'>3.0 TRANSFORM</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-providence",
   "metadata": {},
   "source": [
    "## <font color='blue'>trata os datasets</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-night",
   "metadata": {},
   "source": [
    "### <font color='black'>trata o dataset sla</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "weekly-martial",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:36:36.172033Z",
     "start_time": "2021-08-16T22:36:36.163091Z"
    }
   },
   "outputs": [],
   "source": [
    "def trSLA(df):\n",
    "    \n",
    "    print('df_sla - iniciando transformação... ', end='')\n",
    "    \n",
    "    MINUTOS_PARA_DIAS = 1440 # quantidade de minutos em um dia\n",
    "\n",
    "    # preenche com null as colunas que contém 'null' como tipo string\n",
    "    df.loc[(df['limiteMinimo'] == ''), 'limiteMinimo'] = np.nan\n",
    "    df.loc[(df['limiteMaximo'] == ''), 'limiteMaximo'] = np.nan\n",
    "\n",
    "    # transforma o tipo de coluna para float\n",
    "    df['limiteMinimo'] = df['limiteMinimo'].astype(float)\n",
    "    df['limiteMaximo'] = df['limiteMaximo'].astype(float)\n",
    "\n",
    "    # transforma a unidade de medida de minuto para dias\n",
    "    df['limiteMinimo'] = df['limiteMinimo'] / MINUTOS_PARA_DIAS\n",
    "    df['limiteMaximo'] = df['limiteMaximo'] / MINUTOS_PARA_DIAS\n",
    "\n",
    "    # preenche com valores extremos os limites máximos e limites mínimos\n",
    "    df.loc[(df['limiteMinimo'].isna()), 'limiteMinimo'] = -9999999.9\n",
    "    df.loc[(df['limiteMaximo'].isna()), 'limiteMaximo'] = 9999999.9\n",
    "\n",
    "    # cria mais uma coluna de status para pivotar limite mínimo e limite máxio\n",
    "    df['status2'] = df['status']\n",
    "    df.rename(columns={'status': 'StatusLmin', 'status2': 'StatusLmax'}, inplace = True)\n",
    "    \n",
    "    # faz pivot da coluna StatusLmin\n",
    "    idx = ['entityCode', 'service', 'StatusLmax', 'limiteMaximo']\n",
    "    df = df.pivot(columns = 'StatusLmin', values = 'limiteMinimo', index=idx).reset_index()\n",
    "    df.columns.name = None\n",
    "    dic_renome = {'Dentro do prazo' : 'Dentro do Prazo LMin', \n",
    "                  'Fora do prazo' : 'Fora do Prazo LMin',\n",
    "                  'Perto do prazo' : 'Perto do Prazo LMin'}\n",
    "    df.rename(columns=dic_renome, inplace = True)\n",
    "\n",
    "    # faz pivot da coluna StatusLmax\n",
    "    idx = ['entityCode', 'service', 'Dentro do Prazo LMin', 'Fora do Prazo LMin', 'Perto do Prazo LMin']\n",
    "    df = df.pivot(columns = 'StatusLmax', values = 'limiteMaximo', index=idx).reset_index()\n",
    "    df.columns.name = None\n",
    "    dic_renome = {'Dentro do prazo' : 'Dentro do Prazo LMax', \n",
    "                  'Fora do prazo' : 'Fora do Prazo LMax',\n",
    "                  'Perto do prazo' : 'Perto do Prazo LMax'}\n",
    "    df.rename(columns=dic_renome, inplace = True)\n",
    "\n",
    "    # agrupa por entidade e serviço\n",
    "    df = df.groupby('service').sum().reset_index()\n",
    "\n",
    "    print('%d linhas OK.' %df.shape[0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-pierre",
   "metadata": {},
   "source": [
    "### <font color='black'>trata o dataset form</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "scheduled-auction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:36:37.422498Z",
     "start_time": "2021-08-16T22:36:37.418329Z"
    }
   },
   "outputs": [],
   "source": [
    "def trForm(df):\n",
    "    \n",
    "    print('df_form - iniciando transformação... ', end='')\n",
    "\n",
    "    df.columns = ['atributo', 'nome', 'valor', 'protocolo', 'servico', 'tipo', 'campo_relacionado']\n",
    "    df['atributo'] = df['atributo'].str.lower()\n",
    "    \n",
    "    print('%d linhas OK.' %df.shape[0])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-yukon",
   "metadata": {},
   "source": [
    "### <font color='black'>trata o dataset rating</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dynamic-jason",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:36:39.063189Z",
     "start_time": "2021-08-16T22:36:39.058479Z"
    }
   },
   "outputs": [],
   "source": [
    "def trRating(df):\n",
    "\n",
    "    print('df_rating - iniciando transformação... ', end='')\n",
    "\n",
    "    # renomeia as colunas\n",
    "    cols = ['Solicitacao', 'NotaAvaliacao', 'MotivoAvaliacao', 'DataHoraAvaliacao']\n",
    "    df.columns = cols\n",
    "\n",
    "    # separa e formata as colunas de datas e horas \n",
    "    df['DataAvaliacao'] = pd.to_datetime(df['DataHoraAvaliacao']).dt.date\n",
    "    df['HoraAvaliacao'] = pd.to_datetime(df['DataHoraAvaliacao']).dt.time\n",
    "\n",
    "    # deleta a coluna que contém data e hora\n",
    "    df.drop(['DataHoraAvaliacao'], axis=1, inplace=True)\n",
    "\n",
    "    # preenche colunas de linhas vazias\n",
    "    df.loc[df['MotivoAvaliacao'] == '', 'MotivoAvaliacao'] = '<motivo vazio>'\n",
    "    \n",
    "    print('%d linhas OK.' %df.shape[0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-democrat",
   "metadata": {},
   "source": [
    "### <font color='black'>trata o dataset tasks</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "assumed-blast",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:36:40.592839Z",
     "start_time": "2021-08-16T22:36:40.586057Z"
    }
   },
   "outputs": [],
   "source": [
    "def trTasks(df):\n",
    "\n",
    "    print('df_tasks - iniciando transformação... ', end='')\n",
    "\n",
    "    SEM_STATUS = '<sem status inicial>'\n",
    "\n",
    "    # renomeia as colunas\n",
    "    lst_colunas_tasks = ['Protocolo', 'Entidade', 'Servico', 'Usuarios','Grupo', 'DataHora_Conclusao',\n",
    "                         'DataHora_Criacao', 'Acao', 'EncaminhadoPara', 'ProcessoEncerrado', 'ProcessoCancelado',\n",
    "                         'MotivoCancelamento', 'StatusExterno', 'Categoria', 'GrupoResponsavel','Prazo']\n",
    "    df.columns = lst_colunas_tasks\n",
    "\n",
    "    # coloca null em todas as colunas com dados vazios ou null\n",
    "    df = df.replace(['null', '', '-'], np.nan)\n",
    "\n",
    "    # coloca dataset em ordem para acertar o status externo\n",
    "    lst = ['Entidade', 'Protocolo', 'DataHora_Criacao']\n",
    "    df = df.sort_values(by=lst).reset_index(drop=True)\n",
    "\n",
    "    # acerta os tipos das colunas datetime\n",
    "    df['DataHora_Conclusao'] = df['DataHora_Conclusao'].astype('datetime64')\n",
    "    df['DataHora_Criacao']   = df['DataHora_Criacao'].astype('datetime64')\n",
    "\n",
    "    # rotina para preencher status externo vazio\n",
    "    # pega sempre o anterior e se o primeiro status estiver vazio coloca \"sem status inicial\"\n",
    "    if pd.isna(df.loc[0, 'StatusExterno']):\n",
    "        df.loc[0, 'StatusExterno'] = SEM_STATUS\n",
    "\n",
    "    for i in range(1, df.shape[0]):\n",
    "        if pd.isna(df.loc[i, 'StatusExterno']):    \n",
    "            if df.loc[i, 'Protocolo'] == df.loc[i - 1, 'Protocolo']:\n",
    "                df.loc[i, 'StatusExterno'] = df.loc[i - 1, 'StatusExterno']\n",
    "            else:\n",
    "                df.loc[i, 'StatusExterno'] = SEM_STATUS\n",
    "\n",
    "    print('%d linhas OK.' %df.shape[0])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-calgary",
   "metadata": {},
   "source": [
    "## <font color='blue'>monta tabelas dimensões</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-interim",
   "metadata": {},
   "source": [
    "### DIM acoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "digital-library",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:36:42.404193Z",
     "start_time": "2021-08-16T22:36:42.393727Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_dim_Acoes(df):\n",
    "\n",
    "    print('dim_Acoes - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna Acao em uma coluna do tipo \"category'\n",
    "    df['tmpAcao'] = df['Acao'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Ação\n",
    "    df['FK_dim_Acoes'] = df['tmpAcao'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_Acoes', 'Acao']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_Acao': 'PK_dim_Acao'}, inplace = True)\n",
    "    \n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['Acao'] == '', ['Acao']] = '<sem ação determinada>'\n",
    "    \n",
    "    # exclui a coluna Acoes de df_tasks que será a futura tabela fato\n",
    "    df.drop(['Acao', 'tmpAcao'], axis=1, inplace=True)\n",
    "    \n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-juice",
   "metadata": {},
   "source": [
    "### DIM categoria servicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "medium-teaching",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:36:43.628718Z",
     "start_time": "2021-08-16T22:36:43.623984Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_dim_CategoriaServico(df):\n",
    "\n",
    "    print('dim_Categoria - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna Categoria em uma coluna do tipo \"category'\n",
    "    df['tmpCategoria'] = df['Categoria'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão CategoriasServicos\n",
    "    df['FK_dim_CategoriasServicos'] = df['tmpCategoria'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_CategoriasServicos', 'Categoria']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_CategoriasServicos': 'PK_dim_CategoriasServicos'}, inplace = True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['Categoria'] == '', ['Categoria']] = '<categoria indefinida>'\n",
    "    \n",
    "    # exclui a coluna Categoria de df_tasks que será a futura tabela fato\n",
    "    df.drop(['Categoria', 'tmpCategoria'], axis=1, inplace=True)\n",
    "    \n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-things",
   "metadata": {},
   "source": [
    "### DIM encaminhamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "comic-cookbook",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:36:44.766583Z",
     "start_time": "2021-08-16T22:36:44.761994Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_dim_Encaminhamento(df):\n",
    "\n",
    "    print('dim_Encaminhamento - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna EncaminhadoPara em uma coluna do tipo \"category'\n",
    "    df['tmpEncaminhadoPara'] = df['EncaminhadoPara'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Encaminhamento\n",
    "    df['FK_dim_Encaminhamento'] = df['tmpEncaminhadoPara'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_Encaminhamento', 'EncaminhadoPara']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_Encaminhamento': 'PK_dim_Encaminhamento'}, inplace = True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['EncaminhadoPara'] == '', ['EncaminhadoPara']] = '<sem encaminhamento>'\n",
    "\n",
    "    # exclui a coluna Entidade de df_tasks que será a futura tabela fato\n",
    "    df.drop(['EncaminhadoPara', 'tmpEncaminhadoPara'], axis=1, inplace=True)\n",
    "\n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-portland",
   "metadata": {},
   "source": [
    "### DIM entidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "extraordinary-hungary",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:36:45.942601Z",
     "start_time": "2021-08-16T22:36:45.937720Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_dim_Entidade(df):\n",
    "\n",
    "    print('dim_Entidade - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna Entidade em uma coluna do tipo \"category'\n",
    "    df['tmpEntidade'] = df['Entidade'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Entidades\n",
    "    df['FK_dim_Entidades'] = df['tmpEntidade'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_Entidades', 'Entidade']].drop_duplicates()\n",
    "    dfx['Entidade'] = dfx['Entidade'].str.upper()\n",
    "    dfx.rename(columns={'FK_dim_Entidades': 'PK_dim_Entidades'}, inplace = True)\n",
    "\n",
    "    # exclui a coluna Entidade de df_tasks que será a futura tabela fato\n",
    "    df.drop(['Entidade', 'tmpEntidade'], axis=1, inplace=True)\n",
    "\n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-silicon",
   "metadata": {},
   "source": [
    "### DIM grupo responsavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "central-camera",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:36:47.286615Z",
     "start_time": "2021-08-16T22:36:47.281785Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_dim_GrupoResponsavel(df):\n",
    "\n",
    "    print('dim_GrupoResponsavel - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna GrupoResponsavel em uma coluna do tipo \"category'\n",
    "    df['tmpGrupoResponsavel'] = df['GrupoResponsavel'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão GrupoResponsavel\n",
    "    df['FK_dim_GrupoResponsavel'] = df['tmpGrupoResponsavel'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_GrupoResponsavel', 'GrupoResponsavel']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_GrupoResponsavel': 'PK_dim_GrupoResponsavel'}, inplace = True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['GrupoResponsavel'] == '', ['GrupoResponsavel']] = '<grupo responsável não definido>'\n",
    "\n",
    "    # exclui a coluna GrupoResponsavel de df_tasks que será a futura tabela fato\n",
    "    df.drop(['GrupoResponsavel', 'tmpGrupoResponsavel'], axis=1, inplace=True)\n",
    "    \n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-sample",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-04T11:19:20.012078Z",
     "start_time": "2021-03-04T11:19:20.009504Z"
    }
   },
   "source": [
    "### DIM grupo usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "charged-france",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:36:48.574647Z",
     "start_time": "2021-08-16T22:36:48.569741Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_dim_GruposUsuarios(df):\n",
    "\n",
    "    print('dim_GruposUsuarios - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna Grupo em uma coluna do tipo \"category'\n",
    "    df['tmpGrupo'] = df['Grupo'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Grupo\n",
    "    df['FK_dim_GruposUsuarios'] = df['tmpGrupo'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_GruposUsuarios', 'Grupo']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_GruposUsuarios': 'PK_dim_GruposUsuarios'}, inplace = True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['Grupo'] == '', ['Grupo']] = '<grupo usuário não definido>'\n",
    "\n",
    "    # exclui a coluna Grupo de df_tasks que será a futura tabela fato\n",
    "    df.drop(['Grupo', 'tmpGrupo'], axis=1, inplace=True)\n",
    "    \n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-grenada",
   "metadata": {},
   "source": [
    "### DIM motivos cancelamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "greatest-champagne",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:36:49.718484Z",
     "start_time": "2021-08-16T22:36:49.713747Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_MotivosCanc(df):\n",
    "\n",
    "    print('dim_MotivosCanc - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna MotivoCancelamento em uma coluna do tipo \"category'\n",
    "    df['tmpMotivoCancelamento'] = df['MotivoCancelamento'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão MotivoCanc\n",
    "    df['FK_dim_MotivosCanc'] = df['tmpMotivoCancelamento'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_MotivosCanc', 'MotivoCancelamento']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_MotivosCanc': 'PK_dim_MotivosCanc'}, inplace = True)\n",
    "\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['MotivoCancelamento'] == '', ['MotivoCancelamento']] = '<sem motivo de cancelamento>'\n",
    "\n",
    "    # exclui a coluna MotivoCancelamento de df_tasks que será a futura tabela fato\n",
    "    df.drop(['MotivoCancelamento', 'tmpMotivoCancelamento'], axis=1, inplace=True)\n",
    "\n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-barbados",
   "metadata": {},
   "source": [
    "### DIM servico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "weekly-universal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:36:50.908190Z",
     "start_time": "2021-08-16T22:36:50.902480Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_Servico(df, dff_sla):\n",
    "\n",
    "    print('dim_Servico - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna Servico em uma coluna do tipo \"category'\n",
    "    df['tmpServico'] = df['Servico'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Servico\n",
    "    df['FK_dim_Servicos'] = df['tmpServico'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_Servicos', 'Servico']].drop_duplicates()\n",
    "\n",
    "    # exclui a coluna Servico de df_tasks que será a futura tabela fato\n",
    "    df.drop(['Servico', 'tmpServico'], axis=1, inplace=True)\n",
    "\n",
    "    # defive os nomes das colunas de SLA\n",
    "    cols = {'FK_dim_Servicos': 'PK_dim_Servicos',\n",
    "            'Dentro do Prazo LMin': 'sla_VD_Lmin',\n",
    "            'Perto do Prazo LMin': 'sla_AM_Lmin',\n",
    "            'Fora do Prazo LMin': 'sla_VM_Lmin',\n",
    "            'Dentro do Prazo LMax': 'sla_VD_Lmax',\n",
    "            'Perto do Prazo LMax': 'sla_AM_LMax',\n",
    "            'Fora do Prazo LMax': 'sla_VM_LMax',\n",
    "           }\n",
    "    # faz um merge da dimensão dim_Servicos com a tabela de SLAs\n",
    "    dfx = dfx.merge(dff_sla, left_on='Servico', right_on='service', how='left')\n",
    "    dfx = dfx.drop('service', axis=1)\n",
    "    dfx.rename(columns=cols, inplace = True)\n",
    "    \n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-occurrence",
   "metadata": {},
   "source": [
    "### DIM status externo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "prompt-rebecca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:36:52.046611Z",
     "start_time": "2021-08-16T22:36:52.041915Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_StatusExt(df):\n",
    "\n",
    "    print('dim_StatusExt - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna StatusExterno em uma coluna do tipo \"category'\n",
    "    df['tmpStatusExterno'] = df['StatusExterno'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Grupo\n",
    "    df['FK_dim_StatusExt'] = df['tmpStatusExterno'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_StatusExt', 'StatusExterno']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_StatusExt': 'PK_dim_StatusExt'}, inplace = True)\n",
    "\n",
    "    # exclui a coluna Grupo de df_tasks que será a futura tabela fato\n",
    "    df.drop(['StatusExterno', 'tmpStatusExterno'], axis=1, inplace=True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['StatusExterno'] == '', ['StatusExterno']] = '<sem status>'\n",
    "\n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-textbook",
   "metadata": {},
   "source": [
    "### DIM usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "connected-theology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:36:53.619167Z",
     "start_time": "2021-08-16T22:36:53.614230Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_Usuarios(df):\n",
    "\n",
    "    print('dim_Usuario - iniciando montagem... ', end='')\n",
    "\n",
    "    # trasnforma a coluna Usuario em uma coluna do tipo \"category'\n",
    "    df['tmpUsuario'] = df['Usuarios'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Usuario\n",
    "    df['FK_dim_Usuarios'] = df['tmpUsuario'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_Usuarios', 'Usuarios']].drop_duplicates()\n",
    "    col_ren = {'FK_dim_Usuarios': 'PK_dim_Usuarios', 'Usuarios': 'Usuario'}\n",
    "    dfx.rename(columns=col_ren, inplace = True)\n",
    "\n",
    "    # exclui a coluna Usuario de df_tasks que será a futura tabela fato\n",
    "    df.drop(['Usuarios', 'tmpUsuario'], axis=1, inplace=True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['Usuario'] == '', ['Usuario']] = '<usuário indefinido>'\n",
    "\n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-string",
   "metadata": {},
   "source": [
    "### DIM situacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "previous-third",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:36:55.081282Z",
     "start_time": "2021-08-16T22:36:55.077838Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_Situacao():\n",
    "\n",
    "    print('dim_Situacao - iniciando montagem... ', end='')\n",
    "\n",
    "    dfx = pd.DataFrame({'PK_dim_Situacao': [1, 2, 3],\n",
    "                        'Situacao':        ['em Andamento', 'Encerrada', 'Cancelada']}\n",
    "                        )\n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "\n",
    "    return dfx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-click",
   "metadata": {},
   "source": [
    "### DIM SLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "reported-starter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:36:57.926824Z",
     "start_time": "2021-08-16T22:36:57.922346Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_SLA():\n",
    "\n",
    "    print('dim_SLA - iniciando montagem... ', end='')\n",
    "\n",
    "    dfx = pd.DataFrame({'PK_dim_SLA': [1, 2, 3],\n",
    "                        'DescSLA':    ['dentro do prazo', 'perto do prazo', 'fora do prazo'],\n",
    "                        'Cor':        ['verde','amarelo' ,'vermelho']}\n",
    "                    )\n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-syndicate",
   "metadata": {},
   "source": [
    "### DIM endereco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "broke-degree",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:00.369927Z",
     "start_time": "2021-08-16T22:37:00.360534Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_Endereco(dfe):\n",
    "\n",
    "    print('dim_Endereco - iniciando montagem... ', end='')\n",
    "\n",
    "    dc = ['state', 'city', 'neighborhood', 'zipcode', 'street']\n",
    "    df = dfe.loc[dfe['atributo'].isin(dc), ['atributo','valor', 'protocolo']].copy()\n",
    "# ===================================================    \n",
    "    # faz pivot da coluna atributo\n",
    "    df = df.pivot(columns='atributo', values='valor', index='protocolo').reset_index()\n",
    "    df.columns.name = None\n",
    "\n",
    "    # completa dataset com campos obrigatórios para composição de endereço \n",
    "    for i in dc:\n",
    "        if i not in df.columns:\n",
    "            df[i] = ''\n",
    "    \n",
    "    # renomeia colunas\n",
    "    dic_renome = {'protocolo': 'Solicitacao', \n",
    "                  'zipcode': 'CEP', \n",
    "                  'street': 'Endereco', \n",
    "                  'neighborhood': 'Bairro', \n",
    "                  'city': 'Cidade', \n",
    "                  'state': 'UF'}\n",
    "    df.rename(columns=dic_renome, inplace = True)\n",
    "    \n",
    "    # substitui a coluna Endereco por vazio quando '-' ou null\n",
    "    df.loc[(df['Endereco'] == '-') | (df['Endereco'].isnull()), ['Endereco']] = ''\n",
    "    df.loc[(df['Bairro'] == '-') | (df['Bairro'].isnull()), ['Bairro']] = ''\n",
    "\n",
    "    # cria a coluna Endereco Completo\n",
    "    df['EnderecoCompleto'] = df['Cidade'] + ', ' + df['UF'] + ', Brasil' \n",
    "\n",
    "# ===================================================    \n",
    "\n",
    "    dict_uf = {'AC': 'Acre', 'AL': 'Alagoas', 'AP': 'Amapá', 'AM': 'Amazonas', 'BA': 'Bahia',\n",
    "               'CE': 'Ceará', 'ES': 'Espírito Santo', 'GO': 'Goiás', 'MA': 'Maranhão', 'MT': 'Mato Grosso',\n",
    "               'MS': 'Mato Grosso do Sul', 'MG': 'Minas Gerais', 'PA': 'Pará', 'PB': 'Paraíba',\n",
    "               'PR': 'Paraná', 'PE': 'Pernambuco', 'PI': 'Piauí', 'RJ': 'Rio de Janeiro',\n",
    "               'RN': 'Rio Grande do Norte', 'RS': 'Rio Grande do Sul', 'RO': 'Rondônia', 'RR': 'Roraima',\n",
    "               'SC': 'Santa Catarina', 'SP': 'São Paulo', 'SE': 'Sergipe', 'TO': 'Tocantins', 'DF': 'Distrito Federal',\n",
    "               '<nd>': '<sem UF>'\n",
    "    }\n",
    "\n",
    "    # acrescenta o nome do estado ao dataframe\n",
    "    dfx = df.copy()\n",
    "    dfx['NomeUF'] = dfx['UF'].apply(lambda x: dict_uf.get(x, None))\n",
    "    dfx.rename(columns={'Solicitacao': 'PK_dim_Endereco'}, inplace = True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['CEP'].isna(), ['CEP']] = ''\n",
    "\n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-coordination",
   "metadata": {},
   "source": [
    "### FAT tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "violent-barbados",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:01.853441Z",
     "start_time": "2021-08-16T22:37:01.850373Z"
    }
   },
   "outputs": [],
   "source": [
    "# função retorna a situação da solicitação de acordo com as coilunas cancelado e encerrado\n",
    "def RetSit(Enc, Canc):\n",
    "    rt = 0\n",
    "    if Canc == 1:\n",
    "        rt = 3\n",
    "    elif Enc == 1:\n",
    "        rt = 2\n",
    "    else:\n",
    "        rt = 1\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "valued-purchase",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:03.649095Z",
     "start_time": "2021-08-16T22:37:03.642887Z"
    }
   },
   "outputs": [],
   "source": [
    "def CalculaPrazoSolicitacao(df):\n",
    "    # guarda a data atual\n",
    "    AGORA = pd.Timestamp('today')\n",
    "\n",
    "    # cria uma coluna temporária para calcular o prazo\n",
    "    df['tmp_DataHora_Conclusao'] = df['DataHora_Conclusao']\n",
    "\n",
    "    # na coluna temporária, se não houver data de conclusão colocar a datahora atual\n",
    "    df.loc[df['tmp_DataHora_Conclusao'].isna(), 'tmp_DataHora_Conclusao'] = AGORA\n",
    "\n",
    "    # calcula o prazo da tarefa em dias com decimal\n",
    "    df['prazo_task'] = (df['tmp_DataHora_Conclusao'] - df['DataHora_Criacao']) / pd.to_timedelta(1, unit='D')\n",
    "\n",
    "    # cria um dataframe temporário com a soma de prazos de uma solicitação\n",
    "    dfx = df[['FK_dim_Solicitacoes', 'prazo_task']].groupby('FK_dim_Solicitacoes').sum().reset_index()\n",
    "\n",
    "    # cria uma coluna de soma de prazo da respectiva solicitação\n",
    "    df = df.merge(dfx, how='left', on='FK_dim_Solicitacoes')\n",
    "    \n",
    "    # cria um dataframe temporário com a data da solicitação (menor data entre as tasks)\n",
    "    dfx = df[['FK_dim_Solicitacoes', 'DataHora_Criacao']].groupby('FK_dim_Solicitacoes').min().reset_index()\n",
    "    dfx.columns = ['FK_dim_Solicitacoes', 'DataHora_Solicitacao']\n",
    "    \n",
    "    # cria uma coluna com a data da solicitação\n",
    "    df = df.merge(dfx, how='left', on='FK_dim_Solicitacoes')\n",
    "    \n",
    "    df['DataSolicitacao'] = pd.to_datetime(df['DataHora_Solicitacao']).dt.date\n",
    "    df['HoraSolicitacao'] = pd.to_datetime(df['DataHora_Solicitacao']).dt.time    \n",
    "\n",
    "    # exclui a coluna GrupoResponsavel de df_tasks que será a futura tabela fato\n",
    "    df.drop(['tmp_DataHora_Conclusao', 'DataHora_Solicitacao'], axis=1, inplace=True)\n",
    "    df.rename(columns={'prazo_task_y': 'PrazoSolicitacao', 'prazo_task_x': 'PrazoTask'}, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "racial-religion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:04.734144Z",
     "start_time": "2021-08-16T22:37:04.722909Z"
    }
   },
   "outputs": [],
   "source": [
    "def Mt_Tasks(df, dfr):\n",
    "\n",
    "    print('fat_Tasks - iniciando montagem... ', end='')\n",
    "\n",
    "    dfx = df.copy()\n",
    "    \n",
    "    # faz um merge com o dataset de ratings\n",
    "    dfx = df.merge(dfr, left_on='Protocolo', right_on='Solicitacao', how='left')\n",
    "    dfx.drop(['Solicitacao'], axis=1, inplace=True)\n",
    "\n",
    "    # define o código de situação da solicitação\n",
    "    df_aux = dfx[['Protocolo', 'ProcessoEncerrado', 'ProcessoCancelado']].drop_duplicates()\n",
    "\n",
    "    # substui valores de colunas\n",
    "    troca = {'false': 0, 'true': 1}\n",
    "    df_aux['ProcessoEncerrado'] = df_aux['ProcessoEncerrado'].map(troca)\n",
    "    df_aux['ProcessoCancelado'] = df_aux['ProcessoCancelado'].map(troca)\n",
    "\n",
    "    # agrega as ações para um registro por protocolo\n",
    "    df_aux = df_aux.groupby('Protocolo').agg({'ProcessoEncerrado': 'max', 'ProcessoCancelado': 'max'}).reset_index()\n",
    "\n",
    "    # retorna a situção da solicitação: 1-em andamento, 2-encerrado, 3-cancelado\n",
    "    df_aux['Situacao'] = df_aux.apply(lambda x: RetSit(x['ProcessoEncerrado'], x['ProcessoCancelado']), axis=1)\n",
    "    df_aux.drop(['ProcessoEncerrado', 'ProcessoCancelado'], axis=1, inplace=True)\n",
    "\n",
    "    # faz merge com o dataset de situação da solicitação\n",
    "    dfx = dfx.merge(df_aux, left_on='Protocolo', right_on='Protocolo', how='left')\n",
    "    dfx.drop(['ProcessoEncerrado', 'ProcessoCancelado'], axis=1, inplace=True)\n",
    "\n",
    "    # coloca -1 para as solicitações que não possuem avaliação\n",
    "    dfx.loc[dfx['NotaAvaliacao'].isna(), 'NotaAvaliacao'] = -1\n",
    "    dfx['NotaAvaliacao'] = dfx['NotaAvaliacao'].astype('int32')\n",
    "\n",
    "    # renomeia as colunas\n",
    "    col_ren = {'Protocolo': 'FK_dim_Solicitacoes', 'Situacao': 'FK_dim_Situacao'}\n",
    "    dfx.rename(columns=col_ren, inplace = True)\n",
    "\n",
    "    # trata a coluna Prazo\n",
    "    \n",
    "    dfx.loc[dfx['Prazo'].isna(), 'Prazo'] = '0'\n",
    "#     dfx.loc[(dfx['Prazo'] == '') | (dfx['Prazo'] == 'null'), 'Prazo'] = '0'\n",
    "    dfx['Prazo'] = dfx['Prazo'].astype('int64')\n",
    "\n",
    "    dfx['DataCriacao'] = pd.to_datetime(dfx['DataHora_Criacao']).dt.date\n",
    "    dfx['HoraCriacao'] = pd.to_datetime(dfx['DataHora_Criacao']).dt.time\n",
    "\n",
    "    dfx['DataConclusao'] = pd.to_datetime(dfx['DataHora_Conclusao']).dt.date\n",
    "    dfx['HoraConclusao'] = pd.to_datetime(dfx['DataHora_Conclusao']).dt.time\n",
    "\n",
    "    dfx = CalculaPrazoSolicitacao(dfx)\n",
    "\n",
    "    cols_ordem = ['FK_dim_Solicitacoes', 'Prazo', \n",
    "                  'DataCriacao', 'HoraCriacao', 'DataHora_Criacao',\n",
    "                  'DataConclusao', 'HoraConclusao', 'DataHora_Conclusao',\n",
    "                  'NotaAvaliacao', 'MotivoAvaliacao', \n",
    "                  'DataAvaliacao', 'HoraAvaliacao', \n",
    "                  'DataSolicitacao', 'HoraSolicitacao',\n",
    "                  'PrazoSolicitacao', 'PrazoTask',\n",
    "                  'FK_dim_Entidades', 'FK_dim_Servicos', 'FK_dim_Usuarios', 'FK_dim_GruposUsuarios', \n",
    "                  'FK_dim_Acoes', 'FK_dim_StatusExt', 'FK_dim_CategoriasServicos', 'FK_dim_GrupoResponsavel', \n",
    "                  'FK_dim_MotivosCanc', 'FK_dim_Encaminhamento', 'FK_dim_Situacao']\n",
    "    dfx = dfx[cols_ordem]\n",
    "\n",
    "    print('%d linhas OK.' %dfx.shape[0])\n",
    "\n",
    "    return dfx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-sandwich",
   "metadata": {},
   "source": [
    "### <font color='green'>META BI - Constantes</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "complex-gregory",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:06.899038Z",
     "start_time": "2021-08-16T22:37:06.893543Z"
    }
   },
   "outputs": [],
   "source": [
    "# tabela de decisão de combinação Dimensões e Métricas\n",
    "mtx =  [('STRING_DIM',  'INTEGER_MET', 'N', 'N', 'S', 'S'),\n",
    "        ('STRING_DIM',  'DOUBLE_MET',  'N', 'N', 'S', 'S'),\n",
    "        ('STRING_DIM',  'NULL_MET',    'S', 'S', 'N', 'N'),\n",
    "\n",
    "        ('BOOLEAN_DIM', 'INTEGER_MET', 'S', 'N', 'N', 'N'),\n",
    "        ('BOOLEAN_DIM', 'DOUBLE_MET',  'S', 'N', 'N', 'N'),\n",
    "        ('BOOLEAN_DIM', 'NULL_MET',    'S', 'N', 'N', 'N'),\n",
    "\n",
    "        ('INTEGER_DIM', 'INTEGER_MET', 'S', 'S', 'S', 'S'),\n",
    "        ('INTEGER_DIM', 'DOUBLE_MET',  'N', 'N', 'N', 'N'),\n",
    "        ('INTEGER_DIM', 'NULL_MET',    'S', 'S', 'N', 'N'),\n",
    "\n",
    "        ('DOUBLE_MET',  'INTEGER_MET', 'N', 'N', 'N', 'N'),\n",
    "        ('DOUBLE_MET',  'DOUBLE_MET',  'N', 'N', 'S', 'S'),\n",
    "        ('DOUBLE_MET',  'NULL_MET',    'N', 'N', 'N', 'N'),\n",
    "\n",
    "        ('ID_DIM',      'INTEGER_MET', 'N', 'N', 'S', 'S'),\n",
    "        ('ID_DIM',      'DOUBLE_MET',  'N', 'N', 'S', 'S'),\n",
    "        ('ID_DIM',      'NULL_MET',    'S', 'S', 'N', 'N')\n",
    "        ]\n",
    "# colunas da tabela de decisão de combinação Dimensões e Métricas\n",
    "cols_mtx = ['t_dim', 't_met', 'Count', 'Distinct Count', 'Sum', 'Average']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-scanner",
   "metadata": {},
   "source": [
    "### <font color='green'>META BI - Tabelas Auxiliares</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ordinary-progress",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:09.356151Z",
     "start_time": "2021-08-16T22:37:09.352257Z"
    }
   },
   "outputs": [],
   "source": [
    "def GeraCodDimMet(d, m):\n",
    "    d_dim = {'STRING_DIM': 10, 'BOOLEAN_DIM': 20, 'INTEGER_DIM' :30, 'DOUBLE_DIM': 40, 'ID_DIM': 50}\n",
    "    d_met = {'INTEGER_MET': 1, 'DOUBLE_MET': 2, 'NULL_MET': 3}\n",
    "\n",
    "    v_dim = d_dim[d] if d in d_dim else 0\n",
    "    v_met = d_met[m] if m in d_met else 0\n",
    "    \n",
    "    return v_dim + v_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fatty-batman",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:10.323038Z",
     "start_time": "2021-08-16T22:37:10.313563Z"
    }
   },
   "outputs": [],
   "source": [
    "def Enumerico(s, tipo):\n",
    "    try:\n",
    "        v = np.float64(s) if tipo == 'DOUBLE' else np.int64(s)\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "collaborative-mambo",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:10.981486Z",
     "start_time": "2021-08-16T22:37:10.978211Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_aux_entidade(df):\n",
    "    # recebe df=tasks\n",
    "\n",
    "    print('Z_aux_entidade - iniciando montagem... ', end='')\n",
    "\n",
    "    df_ret = df[['Protocolo', 'Entidade']].drop_duplicates()\n",
    "    df_ret.columns = ['protocolo', 'entidade']\n",
    "    \n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "regulated-gates",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:11.998406Z",
     "start_time": "2021-08-16T22:37:11.993320Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_aux_BASE(df, df2):\n",
    "    # recebe df=form, df2=df_aux_entidade\n",
    "\n",
    "    print('Z_aux_BASE - iniciando montagem... ', end='')\n",
    "\n",
    "    df_ret = df.copy()\n",
    "    \n",
    "    # transforma as strings atributo e campo relacionado em lower case\n",
    "    df_ret[['atributo', 'campo_relacionado']] = df_ret[['atributo', 'campo_relacionado']].apply(lambda x: x.str.lower())\n",
    "\n",
    "    # faz um merge da dimensão df_aux_BASE com a df_auxentidade para adicionar a coluna entidade\n",
    "    # se não existir o protocolo na tasks não considera a linha -- isso é um erro de integridade da base de dados\n",
    "    df_ret = df_ret.merge(df2, on='protocolo')    \n",
    "    \n",
    "    # cria 3 novas colunas vazias\n",
    "    df_ret['MET_atributo'] = np.nan\n",
    "    df_ret['MET_valor']    = np.nan\n",
    "    df_ret['MET_tipo']     = np.nan\n",
    "\n",
    "    # cria uma coluna como PK\n",
    "    df_ret['PK'] = df_ret['entidade'] + '_' + df_ret['servico'] + '_' + \\\n",
    "                        df_ret['atributo'] + '_' + df_ret['protocolo']\n",
    "\n",
    "    # cria uma coluna com o tipo de registro\n",
    "    df_ret['TIPO_REG'] = 'BASE'\n",
    "    \n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "    \n",
    "    return df_ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "limiting-victoria",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:13.083181Z",
     "start_time": "2021-08-16T22:37:13.075334Z"
    }
   },
   "outputs": [],
   "source": [
    "# traramento do tipo INTEGER ou DOUBLE\n",
    "def Z_aux_MET(df):\n",
    "    # recebe df=df_aux_BASE\n",
    "\n",
    "    print('Z_aux_MET e Z_aux_MET_Err - iniciando montagem... ', end='')\n",
    "\n",
    "    # dataset de métricas - usa somente tipo \"INTEGER\" ou \"DOUBLE\"\n",
    "    cols = ['atributo', 'nome', 'valor', 'protocolo', 'servico', 'tipo', 'campo_relacionado', 'entidade']\n",
    "    df_ret = df.loc[(df['tipo'] == 'INTEGER') | (df['tipo'] == 'DOUBLE'), cols].copy()\n",
    "\n",
    "    # os valores estão com formato: ponto para separador de milhar e vírgula para fração\n",
    "    # tira os pontos separador de milhar e substitui vírgula por ponto\n",
    "    df_ret['valor'] = df_ret['valor'].apply(lambda x: x.replace('.', '').replace(',', '.'))\n",
    "\n",
    "    # alimenta uma coluna indicadora que mostra se o valor corresponde ou não ao tipo\n",
    "    df_ret['numerico'] = df_ret.apply(lambda x: Enumerico(x['valor'], x['tipo']), axis=1)\n",
    "\n",
    "    # cria um dataset de linhas cuja o valor não corresponde ao tipo\n",
    "    df_ret_Err = df_ret[df_ret['numerico'] == False].drop('numerico', axis=1)\n",
    "\n",
    "    # cria um dataset de linhas de métricas\n",
    "    df_ret = df_ret[~df_ret['numerico'] == False].drop('numerico', axis=1)\n",
    "\n",
    "    # identifica que esse é um dataset de métricas\n",
    "    df_ret['TIPO_REG'] = 'MET'\n",
    "\n",
    "    # exclui linhas que não seja uma métrica explícita\n",
    "    df_ret = df_ret[~(df_ret['campo_relacionado'] == '')]\n",
    "\n",
    "    # cria a coluna FK para relacionar com o dataset BASE\n",
    "    df_ret['FK'] = df_ret['entidade'] + '_' + df_ret['servico'] + '_' + \\\n",
    "                   df_ret['campo_relacionado'] + '_' + df_ret['protocolo']\n",
    "\n",
    "    # troca o nome de algumas colunas\n",
    "    dc = {'atributo': 'MET_atributo', 'campo_relacionado': 'atributo', 'valor': 'MET_valor', 'tipo': 'MET_tipo'}\n",
    "    df_ret.rename(columns=dc, inplace = True)\n",
    "\n",
    "    # faz um merge com o dataset BASE \n",
    "    df_ret = df_ret.merge(df[['PK', 'valor', 'tipo']], left_on='FK', right_on='PK')\n",
    "    df_ret.drop(['PK', 'FK'], axis=1, inplace=True)\n",
    "    \n",
    "    print('%d e %d linhas OK.' %(df_ret.shape[0], df_ret_Err.shape[0]))\n",
    "\n",
    "    return df_ret, df_ret_Err\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "quarterly-brand",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:14.075406Z",
     "start_time": "2021-08-16T22:37:14.071188Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_aux_DIM(df, df_m):\n",
    "# recebe df=df_aux_BASE, df_m=df_aux_MET\n",
    "\n",
    "    print('Z_aux_DIM - iniciando montagem... ', end='')\n",
    "\n",
    "    # escolee as colunas que farão a cancatenação de dataframes\n",
    "    cols = ['atributo', 'nome', 'valor', 'protocolo', 'servico', 'tipo', \n",
    "            'entidade', 'MET_atributo', 'MET_valor', 'MET_tipo', 'TIPO_REG']\n",
    "    df_ret = df[cols].copy()\n",
    "    df_ret = pd.concat([df_ret, df_m])\n",
    "    \n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "joint-feedback",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:14.725180Z",
     "start_time": "2021-08-16T22:37:14.721649Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_aux_decisao():\n",
    "\n",
    "    print('Z_aux_decisao - iniciando montagem... ', end='')\n",
    "\n",
    "    df_ret = pd.DataFrame(mtx, columns=cols_mtx)\n",
    "    df_ret['Cod_Decisao'] = df_ret.apply(lambda x: GeraCodDimMet(x['t_dim'], x['t_met']), axis=1)\n",
    "    \n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "    \n",
    "    return df_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "scientific-religious",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:15.483047Z",
     "start_time": "2021-08-16T22:37:15.478877Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_aux_DataSolicitacao(df):\n",
    "    # recebe df=tasks\n",
    "\n",
    "    print('Z_aux_DataSolicitacao - iniciando montagem... ', end='')\n",
    "\n",
    "    df_tmp = df[['Protocolo', 'DataHora_Criacao']].copy()\n",
    "    # cria uma coluna só de datas\n",
    "    df_tmp['data_criacao'] = pd.to_datetime(df_tmp['DataHora_Criacao']).dt.date\n",
    "\n",
    "    # pega a menor data de cada número de protocolo\n",
    "    df_ret = df_tmp[['Protocolo', 'data_criacao']].groupby('Protocolo').agg('min').reset_index()\n",
    "    df_ret.columns = ['protocolo', 'data_criacao']\n",
    "    \n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "available-electricity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:16.293500Z",
     "start_time": "2021-08-16T22:37:16.289338Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_aux_pto(df):\n",
    "    # recebe df=df_aux_dec\n",
    "\n",
    "    print('Z_aux_pto - iniciando montagem... ', end='')\n",
    "\n",
    "    # faz uma cópia da tabela de decisão com as colunas necessárias\n",
    "    cols = ['Cod_Decisao'] + cols_mtx[2:]\n",
    "    df_ret = df[cols].copy()\n",
    "\n",
    "    # faz o unpivot\n",
    "    df_ret = pd.melt(df_ret, id_vars=['Cod_Decisao'], \n",
    "                     value_vars=['Count', 'Distinct Count', 'Sum', 'Average'],\n",
    "                     var_name='Atributo')\n",
    "    df_ret = df_ret[df_ret['value'] == 'S'].drop('value', axis=1)\n",
    "\n",
    "    # trasnforma a coluna Atributo em uma coluna do tipo \"category'\n",
    "    df_ret['Atributo'] = df_ret['Atributo'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Operação\n",
    "    df_ret['FK_ZN_dim_OPERACAO'] = df_ret['Atributo'].cat.codes.astype('int64') + 1\n",
    "    \n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-cambodia",
   "metadata": {},
   "source": [
    "### <font color='green'>META BI - Tabelas Dim, Fat e Pto</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "variable-frequency",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:18.139899Z",
     "start_time": "2021-08-16T22:37:18.136574Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_pto_Decisao_Operacao(df):\n",
    "    # recebe df=df_Z_aux_pto\n",
    "\n",
    "    print('Z_pto_decisao_operacao - iniciando montagem... ', end='')\n",
    "\n",
    "    df_ret = df.copy()\n",
    "    \n",
    "    # cria a tabela df_pto_decisao_operacao necessária ao modelo dimensional\n",
    "    df_ret = df_ret[['Cod_Decisao', 'FK_ZN_dim_OPERACAO']]\n",
    "    df_ret.columns = ['FK_dim_DECISAO', 'FK_ZN_dim_OPERACAO']\n",
    "    \n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ceramic-leather",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:19.066860Z",
     "start_time": "2021-08-16T22:37:19.062902Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_dim_DIM(df):\n",
    "    # recebe df=df_aux_DIM\n",
    "\n",
    "    print('Z_dim_DIM - iniciando montagem... ', end='')\n",
    "\n",
    "    df_ret = df.copy()\n",
    "\n",
    "    df_ret = df_ret[['atributo', 'nome']].drop_duplicates().reset_index(drop=True).reset_index()\n",
    "    df_ret.columns = ['PK_Z_dim_DIM', 'dimensao', 'nome']\n",
    "    df_ret['PK_Z_dim_DIM'] += 1\n",
    "    \n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "pursuant-passion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:19.805982Z",
     "start_time": "2021-08-16T22:37:19.802052Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_dim_SERVICO(df):    \n",
    "    # recebe df=df_aux_DIM\n",
    "\n",
    "    print('Z_dim_SERVICO - iniciando montagem... ', end='')\n",
    "\n",
    "    df_ret = df.copy()\n",
    "\n",
    "    df_ret = df_ret['servico'].drop_duplicates().reset_index(drop=True).reset_index()\n",
    "    df_ret.columns = ['PK_Z_dim_SERVICO', 'servico']\n",
    "    df_ret['PK_Z_dim_SERVICO'] += 1\n",
    "    \n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "selective-arlington",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:20.545913Z",
     "start_time": "2021-08-16T22:37:20.542248Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_dim_MET(df):    \n",
    "    # recebe df=df_aux_DIM\n",
    "\n",
    "    print('Z_dim_MET - iniciando montagem... ', end='')\n",
    "\n",
    "    df_ret = df.copy()\n",
    "\n",
    "    df_ret = df_ret.loc[~df_ret['MET_atributo'].isna(), 'MET_atributo'].drop_duplicates()\n",
    "    df_ret = df_ret.reset_index(drop=True).reset_index()\n",
    "    df_ret.columns = ['PK_Z_dim_MET', 'metrica']\n",
    "    df_ret['PK_Z_dim_MET'] += 1\n",
    "\n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hungry-vegetable",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:21.189862Z",
     "start_time": "2021-08-16T22:37:21.186438Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_dim_OPERACAO(df):    \n",
    "    # recebe df=df_aux_pto\n",
    "\n",
    "    print('Z_dim_OPERACAO - iniciando montagem... ', end='')\n",
    "\n",
    "    df_ret = df.copy()\n",
    "\n",
    "    df_ret = df_ret[['FK_ZN_dim_OPERACAO', 'Atributo']].drop_duplicates()\n",
    "    df_ret.columns = ['PK_Z_dim_OPERACAO', 'operacao']\n",
    "\n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "middle-custom",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:21.890381Z",
     "start_time": "2021-08-16T22:37:21.886850Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_dim_DECISAO(df):    \n",
    "    # recebe df=df_aux_dec\n",
    "\n",
    "    print('Z_dim_DECISAO - iniciando montagem... ', end='')\n",
    "\n",
    "    df_ret = df.copy()\n",
    "\n",
    "    df_ret = df_ret[['Cod_Decisao', 't_dim', 't_met']].copy().rename(columns={'Cod_Decisao': 'PK_Z_dim_DECISAO'})\n",
    "\n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "tight-patent",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:22.885488Z",
     "start_time": "2021-08-16T22:37:22.874830Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_fat_META(df, df_dim, df_ser, df_met, df_dts):    \n",
    "    # recebe df=df_Z_dim_DIM, df_ser=df_Z_dim_SERVICO, df_met= df_Z_dim_MET, df_dts=df_Z_aux_DataSolicitacao\n",
    "\n",
    "    print('Z_fat_META - iniciando montagem... ', end='')\n",
    "\n",
    "    # copia o df auxiliar\n",
    "    df_ret = df.copy()\n",
    "\n",
    "    # faz merge com dim_DIM\n",
    "    df_ret = df_ret.merge(df_dim, \n",
    "                          how='left', \n",
    "                          left_on=['atributo', 'nome'], \n",
    "                          right_on=['dimensao', 'nome']).drop(['atributo', 'nome', 'dimensao'], axis=1)\n",
    "    # faz merge com dim_SERVICO\n",
    "    df_ret = df_ret.merge(df_ser, \n",
    "                          how='left', \n",
    "                          left_on='servico', \n",
    "                          right_on='servico').drop('servico', axis=1)\n",
    "    # faz merge com dim_MET\n",
    "    df_ret = df_ret.merge(df_met, \n",
    "                          how='left', \n",
    "                          left_on='MET_atributo', \n",
    "                          right_on='metrica').drop(['MET_atributo', 'metrica'], axis=1)\n",
    "    # faz merge com Z_aux_DataSolicitacao\n",
    "    df_ret = df_ret.merge(df_dts, \n",
    "                          how='left', \n",
    "                          on='protocolo') #.drop(['MET_atributo', 'metrica'], axis=1)\n",
    "\n",
    "    # cria a coluna indicadora de NULL_MET\n",
    "    df_ret['reg_nativo'] = df_ret['MET_tipo'].apply(lambda x: 1 if pd.isna(x) else 0)\n",
    "\n",
    "    # passa as colunas de tipo para a string indicadora de NULL\n",
    "    df_ret.loc[df_ret['MET_tipo'].isna(), 'MET_tipo'] = 'NULL_MET'\n",
    "    df_ret.loc[df_ret['tipo'].isna(), 'tipo'] = 'NULL_DIM'\n",
    "\n",
    "\n",
    "    # substitui valores da coluna tipo\n",
    "    subs_dim = {'STRING':'STRING_DIM', \n",
    "                'INTEGER': 'INTEGER_DIM', \n",
    "                'BOOLEAN':'BOOLEAN_DIM', \n",
    "                'DOUBLE': 'DOUBLE_DIM',\n",
    "                'ID':'ID_DIM'}\n",
    "    subs_met = {'INTEGER': 'INTEGER_MET', \n",
    "                'DOUBLE': 'DOUBLE_MET'}\n",
    "\n",
    "    df_ret['tipo'].replace(subs_dim, inplace=True)\n",
    "    df_ret['MET_tipo'].replace(subs_met, inplace=True)\n",
    "\n",
    "    # acha o código para a tabela de decisão da dupla de tipos como FK para a PK de \"df_aux_dec\"\n",
    "    df_ret['FK_Z_dim_DECISAO'] = df_ret.apply(lambda x: GeraCodDimMet(x['tipo'], x['MET_tipo']), axis=1)\n",
    "\n",
    "    # exclui colunas já usadas\n",
    "    df_ret.drop(['tipo', 'MET_tipo'], axis=1, inplace=True)\n",
    "\n",
    "    # acerta os nomes das colunas\n",
    "    dc = {'PK_Z_dim_DIM': 'FK_Z_dim_DIM', \n",
    "          'PK_Z_dim_SERVICO': 'FK_Z_dim_SERVICO', \n",
    "          'PK_Z_dim_MET': 'FK_Z_dim_MET', \n",
    "          'data_criacao': 'FK_Z_Data_Criacao'}\n",
    "    df_ret.rename(columns=dc, inplace = True)\n",
    "\n",
    "    # acerta os tipos das colunas\n",
    "    df_ret['FK_Z_dim_MET'] = df_ret['FK_Z_dim_MET'].astype('Int64')\n",
    "    df_ret['MET_valor'] = df_ret['MET_valor'].astype('float64')\n",
    "    df_ret['FK_Z_Data_Criacao'] = df_ret['FK_Z_Data_Criacao'].apply(pd.to_datetime)\n",
    "\n",
    "    print('%d linhas OK.' %df_ret.shape[0])\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-break",
   "metadata": {},
   "source": [
    "# <font color='red'>4.0 LOAD</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "automated-appointment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:25.951712Z",
     "start_time": "2021-08-16T22:37:25.946214Z"
    }
   },
   "outputs": [],
   "source": [
    "def Regras():\n",
    "    tp_dfs = (\n",
    "        (df_dim_Acao,               'dim_acoes',                 'replace'),               \n",
    "        (df_dim_Categoria,          'dim_categorias_servicos',   'replace'), \n",
    "        (df_dim_Encaminhamento,     'dim_encaminhamento',        'replace'),      \n",
    "        (df_dim_Entidade,           'dim_entidades',             'replace'),           \n",
    "        (df_dim_GrupoResponsavel,   'dim_grupo_responsavel',     'replace'),   \n",
    "        (df_dim_Grupo,              'dim_grupos_usuarios',       'replace'),     \n",
    "        (df_dim_MotivoCanc,         'dim_motivos_canc',          'replace'),        \n",
    "        (df_dim_Servicos,           'dim_servicos',              'replace'),            \n",
    "        (df_dim_StatusExt,          'dim_status_ext',            'replace'),          \n",
    "        (df_dim_Usuarios,           'dim_usuarios',              'replace'),            \n",
    "        (df_dim_Situacao,           'dim_situacao',              'replace'),            \n",
    "        (df_dim_SLA,                'dim_sla',                   'replace'),                 \n",
    "        (df_dim_Endereco,           'dim_endereco',              'replace'),            \n",
    "        (df_fat_tasks,              'fat_tasks',                 'replace'),               \n",
    "        (df_Z_pto_decisao_operacao, 'z_pto_decisao_operacao',    'replace'),  \n",
    "        (df_Z_dim_DIM,              'z_dim_dim',                 'replace'),  \n",
    "        (df_Z_dim_SERVICO,          'z_dim_servico',             'replace'),  \n",
    "        (df_Z_dim_MET,              'z_dim_met',                 'replace'),  \n",
    "        (df_Z_dim_OPERACAO,         'z_dim_operacao',            'replace'),  \n",
    "        (df_Z_dim_DECISAO,          'z_dim_decisao',             'replace'),  \n",
    "        (df_Z_fat_META,             'z_fat_meta',                'replace'),\n",
    "        (df_ts,                     'ctl_carga',                 'append')\n",
    "    )\n",
    "    return tp_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "polished-cleanup",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:26.901174Z",
     "start_time": "2021-08-16T22:37:26.893798Z"
    }
   },
   "outputs": [],
   "source": [
    "def Load_BD(strAut, banco):\n",
    "\n",
    "    # import the module\n",
    "    # create sqlalchemy engine\n",
    "    \n",
    "    if banco == 'mysql':\n",
    "        bd_load = \"mysql+pymysql://{user}:{pw}@{host}/{db}\"\n",
    "    elif banco == 'postgresql':\n",
    "        bd_load = \"postgresql://{user}:{pw}@{host}/{db}\"\n",
    "    else:\n",
    "        print('Nome de Banco de Dados {} Inválido'.format(banco));\n",
    "        return 1\n",
    "\n",
    "    print('LoadBD - carregando tabelas no BD {} ... '.format(banco))\n",
    "\n",
    "    SQLengine = create_engine(bd_load\n",
    "                           .format(user = strAut['My_user'],\n",
    "                                   pw   = strAut['My_pw'],\n",
    "                                   host = strAut['My_host'],\n",
    "                                   db   = strAut['My_db']),\n",
    "                                   pool_recycle=3600)\n",
    "    dbConn = SQLengine.connect()\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for i in Regras():\n",
    "            print('    carregando tabela {} ... '.format(i[1]), end='')\n",
    "            i[0].to_sql(i[1], con=dbConn, if_exists=i[2], index=False, chunksize = 10000)\n",
    "            print('OK {0: .2f}'.format((time.time() - start_time)))\n",
    "        print('LoadBD - carga de tabelas OK')\n",
    "        \n",
    "    except ValueError as vx:\n",
    "        print('ERROR -', vx)\n",
    "\n",
    "    except Exception as ex: \n",
    "        print('EXCEPTION -', ex)\n",
    "\n",
    "    else:\n",
    "        print('Tabelas criadas com sucesso');  \n",
    "\n",
    "    finally:\n",
    "        dbConn.close()\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-asian",
   "metadata": {},
   "source": [
    "# <font color='red'>0.0 MAIN</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "contemporary-leone",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:28.519488Z",
     "start_time": "2021-08-16T22:37:28.514305Z"
    }
   },
   "outputs": [],
   "source": [
    "# monta as strings de conexão\n",
    "def StrConn(sit):\n",
    "    host = 'localhost'; user = 'gd'; pw = 'Cavaquinho@Dourado@6390'\n",
    "    \n",
    "    \n",
    "    in_prd  = {'My_host': host, 'My_db': 'bd_fontes',        'My_user': user, 'My_pw': pw}\n",
    "    out_prd = {'My_host': host, 'My_db': 'bd_dw',            'My_user': user, 'My_pw': pw}\n",
    "    in_hom  = {'My_host': host, 'My_db': 'bd_hom_fontes',    'My_user': user, 'My_pw': pw}\n",
    "    out_hom = {'My_host': host, 'My_db': 'bd_hom_dw',        'My_user': user, 'My_pw': pw}\n",
    "    in_dev  = {'My_host': host, 'My_db': 'bd_dev_fontes',    'My_user': user, 'My_pw': pw}\n",
    "    out_dev = {'My_host': host, 'My_db': 'bd_dev_dw',        'My_user': user, 'My_pw': pw}\n",
    "    in_sim  = {'My_host': host, 'My_db': 'bd_sim_fontes',    'My_user': user, 'My_pw': pw}\n",
    "    out_sim = {'My_host': host, 'My_db': 'bd_sim_dw',        'My_user': user, 'My_pw': pw}\n",
    "\n",
    "    # retorna o ambiente configurado\n",
    "    if sit == 0:\n",
    "        rt_in  = in_prd\n",
    "        rt_out = out_prd\n",
    "    elif sit == 1:\n",
    "        rt_in  = in_hom\n",
    "        rt_out = out_hom\n",
    "    elif sit == 2:\n",
    "        rt_in  = in_dev\n",
    "        rt_out = out_dev\n",
    "    else:\n",
    "        rt_in  = in_sim\n",
    "        rt_out = out_sim\n",
    "\n",
    "    return rt_in, rt_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "combined-province",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T22:37:29.689735Z",
     "start_time": "2021-08-16T22:37:29.686447Z"
    }
   },
   "outputs": [],
   "source": [
    "def gd_timestamp():\n",
    "    lst = {'DataHora': [datetime.today()], 'Versao': [ETL_VERSAO]}\n",
    "    df = pd.DataFrame(lst)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "instructional-campbell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T23:07:47.601762Z",
     "start_time": "2021-08-16T22:37:31.659263Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando ETL de simulação\n",
      "Iniciando leitura de dados nativos...OK\n",
      "tasks:3293075 registros lidos em 16 colunas\n",
      "sla: 942 registros lidos em 5 colunas\n",
      "form: 5665808 registros lidos em 7 colunas\n",
      "rating: 211707 registros lidos em 4 colunas\n",
      "MySQL connection is closed\n",
      "df_sla - iniciando transformação... 159 linhas OK.\n",
      "df_form - iniciando transformação... 5665808 linhas OK.\n",
      "df_rating - iniciando transformação... 211707 linhas OK.\n",
      "df_tasks - iniciando transformação... 3293075 linhas OK.\n",
      "dim_Acoes - iniciando montagem... 101 linhas OK.\n",
      "dim_Categoria - iniciando montagem... 71 linhas OK.\n",
      "dim_Encaminhamento - iniciando montagem... 94 linhas OK.\n",
      "dim_Entidade - iniciando montagem... 21 linhas OK.\n",
      "dim_GrupoResponsavel - iniciando montagem... 55 linhas OK.\n",
      "dim_GruposUsuarios - iniciando montagem... 49 linhas OK.\n",
      "dim_MotivosCanc - iniciando montagem... 21 linhas OK.\n",
      "dim_Servico - iniciando montagem... 159 linhas OK.\n",
      "dim_StatusExt - iniciando montagem... 98 linhas OK.\n",
      "dim_Usuario - iniciando montagem... 96 linhas OK.\n",
      "dim_Situacao - iniciando montagem... 3 linhas OK.\n",
      "dim_SLA - iniciando montagem... 3 linhas OK.\n",
      "dim_Endereco - iniciando montagem... 433264 linhas OK.\n",
      "fat_Tasks - iniciando montagem... 3293075 linhas OK.\n",
      "Z_aux_entidade - iniciando montagem... 433264 linhas OK.\n",
      "Z_aux_BASE - iniciando montagem... 5665808 linhas OK.\n",
      "Z_aux_MET e Z_aux_MET_Err - iniciando montagem... 399486 e 0 linhas OK.\n",
      "Z_aux_DIM - iniciando montagem... 6065294 linhas OK.\n",
      "Z_aux_decisao - iniciando montagem... 15 linhas OK.\n",
      "Z_aux_DataSolicitacao - iniciando montagem... 433264 linhas OK.\n",
      "Z_aux_pto - iniciando montagem... 23 linhas OK.\n",
      "Z_pto_decisao_operacao - iniciando montagem... 23 linhas OK.\n",
      "Z_dim_DIM - iniciando montagem... 239 linhas OK.\n",
      "Z_dim_SERVICO - iniciando montagem... 159 linhas OK.\n",
      "Z_dim_MET - iniciando montagem... 7 linhas OK.\n",
      "Z_dim_OPERACAO - iniciando montagem... 4 linhas OK.\n",
      "Z_dim_DECISAO - iniciando montagem... 15 linhas OK.\n",
      "Z_fat_META - iniciando montagem... 6065294 linhas OK.\n",
      "LoadBD - carregando tabelas no BD mysql ... \n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymysql'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\4/ipykernel_6632/596364717.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m# LOAD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoad_BD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAUTENTIC_OUT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mysql'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoad_BD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAUTENTIC_OUT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'postgresql'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\4/ipykernel_6632/1562290971.py\u001b[0m in \u001b[0;36mLoad_BD\u001b[1;34m(strAut, banco)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LoadBD - carregando tabelas no BD {} ... '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbanco\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     SQLengine = create_engine(bd_load\n\u001b[0m\u001b[0;32m     17\u001b[0m                            .format(user = strAut['My_user'],\n\u001b[0;32m     18\u001b[0m                                    \u001b[0mpw\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mstrAut\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'My_pw'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36mcreate_engine\u001b[1;34m(url, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\util\\deprecations.py\u001b[0m in \u001b[0;36mwarned\u001b[1;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m                         \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m                     )\n\u001b[1;32m--> 298\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\engine\\create.py\u001b[0m in \u001b[0;36mcreate_engine\u001b[1;34m(url, **kwargs)\u001b[0m\n\u001b[0;32m    558\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m                 \u001b[0mdbapi_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpop_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m         \u001b[0mdbapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdialect_cls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdbapi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdbapi_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[0mdialect_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dbapi\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdbapi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\dialects\\mysql\\pymysql.py\u001b[0m in \u001b[0;36mdbapi\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdbapi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pymysql\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_connect_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_translate_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pymysql'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 0-produção, 1-homologação, 2-desenvolvimento, 3-simulação\n",
    "    dic_tipo = {0: 'produção', 1: 'homologação', 2: 'desenvolvimento', 3: 'simulação'}\n",
    "    etl_tipo = 3\n",
    "    \n",
    "    print(f'Iniciando ETL de {dic_tipo[etl_tipo]}')\n",
    "#     print('Iniciando ETL {0}'.format(dic_tipo[etl_tipo]))\n",
    "    \n",
    "    # EXTRACT\n",
    "    AUTENTIC_IN, AUTENTIC_OUT = StrConn(etl_tipo)\n",
    "    df_tasks, df_sla, df_form, df_rating = LeFontes(AUTENTIC_IN)\n",
    "    \n",
    "    DF_TESTE = df_sla.copy()\n",
    "    \n",
    "    # TRANSFORM\n",
    "    # trata os datasets\n",
    "    df_sla    = trSLA(df_sla)\n",
    "    df_form   = trForm(df_form)\n",
    "    df_rating = trRating(df_rating)\n",
    "    df_tasks  = trTasks(df_tasks)\n",
    "    \n",
    "    dft = df_tasks.copy()\n",
    "    dff = df_form.copy()\n",
    "    \n",
    "    # monta as tabelas dimensão\n",
    "    df_dim_Acao              = Mt_dim_Acoes(df_tasks)\n",
    "    df_dim_Categoria         = Mt_dim_CategoriaServico(df_tasks)\n",
    "    df_dim_Encaminhamento    = Mt_dim_Encaminhamento(df_tasks)\n",
    "    df_dim_Entidade          = Mt_dim_Entidade(df_tasks)\n",
    "    df_dim_GrupoResponsavel  = Mt_dim_GrupoResponsavel(df_tasks)\n",
    "    df_dim_Grupo             = Mt_dim_GruposUsuarios(df_tasks)\n",
    "    df_dim_MotivoCanc        = Mt_MotivosCanc(df_tasks)\n",
    "    df_dim_Servicos          = Mt_Servico(df_tasks, df_sla)\n",
    "    df_dim_StatusExt         = Mt_StatusExt(df_tasks)\n",
    "    df_dim_Usuarios          = Mt_Usuarios(df_tasks)\n",
    "    df_dim_Situacao          = Mt_Situacao()\n",
    "    df_dim_SLA               = Mt_SLA()\n",
    "    df_dim_Endereco          = Mt_Endereco(df_form)\n",
    "    \n",
    "    # monta a tabela fato\n",
    "    df_fat_tasks             = Mt_Tasks(df_tasks, df_rating)\n",
    "    \n",
    "    # monta as tabelas auxiliares do META BI\n",
    "    df_Z_aux_entidade              = Z_aux_entidade(dft)\n",
    "    df_Z_aux_BASE                  = Z_aux_BASE(dff, df_Z_aux_entidade)\n",
    "    df_Z_aux_MET, df_Z_aux_MET_Err = Z_aux_MET(df_Z_aux_BASE)\n",
    "    df_Z_aux_DIM                   = Z_aux_DIM(df_Z_aux_BASE, df_Z_aux_MET)\n",
    "    df_Z_aux_decisao               = Z_aux_decisao()\n",
    "    df_Z_aux_DataSolicitacao       = Z_aux_DataSolicitacao(dft)\n",
    "    df_Z_aux_pto                   = Z_aux_pto(df_Z_aux_decisao)\n",
    "    \n",
    "    # monta as tabelas dimensão do META BI\n",
    "    df_Z_pto_decisao_operacao      = Z_pto_Decisao_Operacao(df_Z_aux_pto)\n",
    "    df_Z_dim_DIM                   = Z_dim_DIM(df_Z_aux_DIM)\n",
    "    df_Z_dim_SERVICO               = Z_dim_SERVICO(df_Z_aux_DIM)\n",
    "    df_Z_dim_MET                   = Z_dim_MET(df_Z_aux_DIM)\n",
    "    df_Z_dim_OPERACAO              = Z_dim_OPERACAO(df_Z_aux_pto)\n",
    "    df_Z_dim_DECISAO               = Z_dim_DECISAO(df_Z_aux_decisao)\n",
    "\n",
    "    # monta as tabelas dimensão do META BI\n",
    "    df_Z_fat_META                  = Z_fat_META(df_Z_aux_DIM,\n",
    "                                                df_Z_dim_DIM, \n",
    "                                                df_Z_dim_SERVICO, \n",
    "                                                df_Z_dim_MET, \n",
    "                                                df_Z_aux_DataSolicitacao)\n",
    "    \n",
    "    # monta data e hora e versão da atualização\n",
    "    df_ts = gd_timestamp()\n",
    "\n",
    "    # LOAD\n",
    "    ret = Load_BD(AUTENTIC_OUT, 'mysql')\n",
    "    ret = Load_BD(AUTENTIC_OUT, 'postgresql')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ded47eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoadBD - carregando tabelas no BD mysql ... \n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(pymysql.err.OperationalError) (2003, \"Can't connect to MySQL server on 'Dourado@6390@localhost' ([Errno 11003] getaddrinfo failed)\")\n(Background on this error at: https://sqlalche.me/e/14/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\pymysql\\connections.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self, sock)\u001b[0m\n\u001b[0;32m    612\u001b[0m                         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 613\u001b[1;33m                             sock = socket.create_connection(\n\u001b[0m\u001b[0;32m    614\u001b[0m                                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect_timeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    786\u001b[0m     \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    917\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11003] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_wrap_pool_connect\u001b[1;34m(self, fn, connection)\u001b[0m\n\u001b[0;32m   3239\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3240\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3241\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mdialect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdbapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\pool\\base.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    309\u001b[0m         \"\"\"\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_ConnectionFairy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\pool\\base.py\u001b[0m in \u001b[0;36m_checkout\u001b[1;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[0;32m    867\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfairy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m             \u001b[0mfairy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ConnectionRecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\pool\\base.py\u001b[0m in \u001b[0;36mcheckout\u001b[1;34m(cls, pool)\u001b[0m\n\u001b[0;32m    475\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheckout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m         \u001b[0mrec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\pool\\impl.py\u001b[0m in \u001b[0;36m_do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    145\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msafe_reraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dec_overflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\util\\langhelpers.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                 compat.raise_(\n\u001b[0m\u001b[0;32m     71\u001b[0m                     \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\util\\compat.py\u001b[0m in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\pool\\impl.py\u001b[0m in \u001b[0;36m_do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\pool\\base.py\u001b[0m in \u001b[0;36m_create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_ConnectionRecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\pool\\base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, pool, connect)\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconnect\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__connect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalize_callback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\pool\\base.py\u001b[0m in \u001b[0;36m__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    665\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msafe_reraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m                 \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error on connect(): %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\util\\langhelpers.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                 compat.raise_(\n\u001b[0m\u001b[0;32m     71\u001b[0m                     \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\util\\compat.py\u001b[0m in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\pool\\base.py\u001b[0m in \u001b[0;36m__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    660\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstarttime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdbapi_connection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invoke_creator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    662\u001b[0m             \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Created new connection %r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\engine\\create.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(connection_record)\u001b[0m\n\u001b[0;32m    589\u001b[0m                         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdialect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\engine\\default.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[1;31m# inherits the docstring from interfaces.Dialect.connect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdbapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\pymysql\\connections.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, user, password, host, database, unix_socket, port, charset, sql_mode, read_default_file, conv, use_unicode, client_flag, cursorclass, init_command, connect_timeout, read_default_group, autocommit, local_infile, max_allowed_packet, defer_connect, auth_plugin_map, read_timeout, write_timeout, bind_address, binary_prefix, program_name, server_public_key, ssl, ssl_ca, ssl_cert, ssl_disabled, ssl_key, ssl_verify_cert, ssl_verify_identity, compress, named_pipe, passwd, db)\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\pymysql\\connections.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self, sock)\u001b[0m\n\u001b[0;32m    663\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: (2003, \"Can't connect to MySQL server on 'Dourado@6390@localhost' ([Errno 11003] getaddrinfo failed)\")",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\4/ipykernel_6632/3640711380.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoad_BD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAUTENTIC_OUT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mysql'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\4/ipykernel_6632/1562290971.py\u001b[0m in \u001b[0;36mLoad_BD\u001b[1;34m(strAut, banco)\u001b[0m\n\u001b[0;32m     20\u001b[0m                                    db   = strAut['My_db']),\n\u001b[0;32m     21\u001b[0m                                    pool_recycle=3600)\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mdbConn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSQLengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self, close_with_result)\u001b[0m\n\u001b[0;32m   3192\u001b[0m         \"\"\"\n\u001b[0;32m   3193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3194\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_connection_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose_with_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclose_with_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3196\u001b[0m     @util.deprecated(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, engine, connection, close_with_result, _branch_from, _execution_options, _dispatch, _has_events, _allow_revalidate)\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m                 \u001b[1;32melse\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m             )\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36mraw_connection\u001b[1;34m(self, _connection)\u001b[0m\n\u001b[0;32m   3271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3272\u001b[0m         \"\"\"\n\u001b[1;32m-> 3273\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_pool_connect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_connection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_wrap_pool_connect\u001b[1;34m(self, fn, connection)\u001b[0m\n\u001b[0;32m   3241\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mdialect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdbapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3242\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3243\u001b[1;33m                 Connection._handle_dbapi_exception_noconnection(\n\u001b[0m\u001b[0;32m   3244\u001b[0m                     \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdialect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3245\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception_noconnection\u001b[1;34m(cls, e, dialect, engine)\u001b[0m\n\u001b[0;32m   2095\u001b[0m             \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewraise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_traceback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2096\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mshould_wrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2097\u001b[1;33m             util.raise_(\n\u001b[0m\u001b[0;32m   2098\u001b[0m                 \u001b[0msqlalchemy_exception\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_traceback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2099\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\util\\compat.py\u001b[0m in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[1;31m# credit to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_wrap_pool_connect\u001b[1;34m(self, fn, connection)\u001b[0m\n\u001b[0;32m   3238\u001b[0m         \u001b[0mdialect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdialect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3239\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3240\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3241\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mdialect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdbapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3242\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\pool\\base.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m         \"\"\"\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_ConnectionFairy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_return_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\pool\\base.py\u001b[0m in \u001b[0;36m_checkout\u001b[1;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[0;32m    866\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_checkout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreadconns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfairy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfairy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m             \u001b[0mfairy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ConnectionRecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m             \u001b[0mfairy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\pool\\base.py\u001b[0m in \u001b[0;36mcheckout\u001b[1;34m(cls, pool)\u001b[0m\n\u001b[0;32m    474\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheckout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m         \u001b[0mrec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m             \u001b[0mdbapi_connection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\pool\\impl.py\u001b[0m in \u001b[0;36m_do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msafe_reraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dec_overflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\util\\langhelpers.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exc_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# remove potential circular references\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                 compat.raise_(\n\u001b[0m\u001b[0;32m     71\u001b[0m                     \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                     \u001b[0mwith_traceback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexc_tb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\util\\compat.py\u001b[0m in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[1;31m# credit to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\pool\\impl.py\u001b[0m in \u001b[0;36m_do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inc_overflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msafe_reraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\pool\\base.py\u001b[0m in \u001b[0;36m_create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;34m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_ConnectionRecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_invalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_checkin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\pool\\base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, pool, connect)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconnect\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__connect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalize_callback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\pool\\base.py\u001b[0m in \u001b[0;36m__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msafe_reraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m                 \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error on connect(): %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;31m# in SQLAlchemy 1.4 the first_connect event is not used by\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\util\\langhelpers.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exc_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# remove potential circular references\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                 compat.raise_(\n\u001b[0m\u001b[0;32m     71\u001b[0m                     \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                     \u001b[0mwith_traceback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexc_tb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\util\\compat.py\u001b[0m in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[1;31m# credit to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\pool\\base.py\u001b[0m in \u001b[0;36m__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    659\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstarttime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdbapi_connection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invoke_creator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    662\u001b[0m             \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Created new connection %r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfresh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\engine\\create.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(connection_record)\u001b[0m\n\u001b[0;32m    588\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m                         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdialect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mcreator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpop_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"creator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\sqlalchemy\\engine\\default.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mcargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[1;31m# inherits the docstring from interfaces.Dialect.connect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdbapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_connect_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\pymysql\\connections.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, user, password, host, database, unix_socket, port, charset, sql_mode, read_default_file, conv, use_unicode, client_flag, cursorclass, init_command, connect_timeout, read_default_group, autocommit, local_infile, max_allowed_packet, defer_connect, auth_plugin_map, read_timeout, write_timeout, bind_address, binary_prefix, program_name, server_public_key, ssl, ssl_ca, ssl_cert, ssl_disabled, ssl_key, ssl_verify_cert, ssl_verify_identity, compress, named_pipe, passwd, db)\u001b[0m\n\u001b[0;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_Alpar_GD\\lib\\site-packages\\pymysql\\connections.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self, sock)\u001b[0m\n\u001b[0;32m    662\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[1;31m# If e is neither DatabaseError or IOError, It's a bug.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: (pymysql.err.OperationalError) (2003, \"Can't connect to MySQL server on 'Dourado@6390@localhost' ([Errno 11003] getaddrinfo failed)\")\n(Background on this error at: https://sqlalche.me/e/14/e3q8)"
     ]
    }
   ],
   "source": [
    "ret = Load_BD(AUTENTIC_OUT, 'mysql')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-prototype",
   "metadata": {},
   "source": [
    "# TESTE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
